{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -O ml-100k.zip https://tinyurl.com/y5ynqofz\n",
    "!unzip ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al byoc/smore/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd byoc/smore/\n",
    "./build_and_push.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./byoc/smore/build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./byoc/smore/dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "user_path = './ml-100k/u.user'\n",
    "item_path = './ml-100k/u.item'\n",
    "user_item = './ml-100k/u.data'\n",
    "\n",
    "user_df = pd.read_csv(user_path, names=['uid','age','gender','occupation','zipcode'],  sep='|')\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['unknown','Action' , 'Adventure', 'Animation', 'Childrens' , 'Comedy' , 'Crime', \\\n",
    "                                        'Documentary', 'Drama' ,'Fantasy' , 'Film-Noir' , 'Horror' , 'Musical', \\\n",
    "                                        'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "\n",
    "item_df = pd.read_csv(item_path, names=['iid','title','release_date','video_release_date', 'imdb url'] + genres,  sep='|', encoding = \"ISO-8859-1\")\n",
    "\n",
    "item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df = pd.read_csv(user_item, names=['iid', 'uid', 'rating', 'timestamp'], sep='\\t')\n",
    "user_item_df = user_item_df.merge(item_df, on=['iid'])\n",
    "user_item_df = user_item_df.merge(user_df, on=['uid'])\n",
    "user_item_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heat_map(df, figsize=(10,7)): \n",
    "    df = df.div(df.sum(axis=1), axis=0)     \n",
    "    plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(df)\n",
    "\n",
    "gender = user_item_df[user_item_df['rating']>3][['gender']+genres].groupby(['gender']).sum()\n",
    "plot_heat_map(gender, figsize=(10,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "occupation = user_item_df[user_item_df['rating']>3][['occupation']+genres].groupby(['occupation']).sum()\n",
    "plot_heat_map(occupation, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df['age_segment']=user_item_df['age']//10\n",
    "age = user_item_df[user_item_df['rating']>3][['age_segment']+genres].groupby(['age_segment']).sum()\n",
    "plot_heat_map(age, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from preprocessing.imdb_datareader import IMDBDataReader\n",
    "from preprocessing.factorization_machine_transformer import  FactorizationMachineTransformer\n",
    "\n",
    "user_path = './ml-100k/u.user'\n",
    "item_path = './ml-100k/u.item'\n",
    "user_item = './ml-100k/u.data'\n",
    "reader = IMDBDataReader()\n",
    "user_item  = reader.read_user_item_rating(user_item)\n",
    "users = reader.read_user_data(user_path)\n",
    "items = reader.read_item_data(item_path)\n",
    "train_user_item = user_item[:int(len(user_item)*0.8)]\n",
    "test_user_item = user_item[int(len(user_item)*0.8):]\n",
    "transformer = FactorizationMachineTransformer(users, items, train_user_item)\n",
    "X_train, Y_train, _, _, nFeatures = transformer.get_feature_vectors(users, items, train_user_item)\n",
    "X_test, Y_test,X_cold_test, Y_cold_test, nFeatures = transformer.get_feature_vectors(users, items, test_user_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3api create-bucket --bucket recommendation-demo-yianc-0814 --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'recommendation-demo-yianc'\n",
    "prefix = 'sagemaker/fm-movielens'\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}/'.format(prefix, 'test')\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "\n",
    "import io,boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeDatasetToProtobuf(X, bucket, prefix, key, d_type, Y=None):\n",
    "    buf = io.BytesIO()\n",
    "    if d_type == \"sparse\":\n",
    "        smac.write_spmatrix_to_sparse_tensor(buf, X, labels=Y)\n",
    "    else:\n",
    "        smac.write_numpy_to_dense_tensor(buf, X, labels=Y)\n",
    "        \n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    " \n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, bucket, train_prefix, train_key, \"sparse\", Y_train)    \n",
    "  \n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "\n",
    "container = sagemaker.image_uris.retrieve(framework='factorization-machines', region='us-east-1', version='latest')\n",
    "container \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import boto3 \n",
    "\n",
    "region = boto3.Session().region_name\n",
    "crole = 'AmazonSageMaker-ExecutionRole-20200603T105247'\n",
    "\n",
    "print(sagemaker.estimator.Estimator)\n",
    "fm = sagemaker.estimator.Estimator(image_uri=container,\n",
    "                                   role=crole, \n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "                      feature_dim=nFeatures,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=64,\n",
    "                      epochs=50)\n",
    "\n",
    "fm.fit({'train': train_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "model_file_name = \"model.tar.gz\"\n",
    "model_full_path = fm.output_path +\"/\"+ fm.latest_training_job.job_name +\"/output/\"+model_file_name\n",
    "print (\"Model Path: \", model_full_path)\n",
    "\n",
    "#Download FM model \n",
    "os.system(\"aws s3 cp \"+model_full_path+ \" .\")\n",
    "\n",
    "#Extract model file for loading to MXNet\n",
    "os.system(\"tar xzvf \"+model_file_name)\n",
    "os.system(\"unzip -o model_algo-1\")\n",
    "os.system(\"mv symbol.json model-symbol.json\")\n",
    "os.system(\"mv params model-0000.params\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract model data\n",
    "import mxnet as mx\n",
    "import numpy as np \n",
    "\n",
    "nb_users = len(transformer.u_idx) \n",
    "nb_movies = len(transformer.i_idx) \n",
    "m = mx.module.Module.load('./model', 0, False, label_names=['out_label'])\n",
    "\n",
    "\n",
    "V = m._arg_params['v'].asnumpy()\n",
    "w = m._arg_params['w1_weight'].asnumpy()\n",
    "b = m._arg_params['w0_weight'].asnumpy()\n",
    "\n",
    "# item latent matrix - concat(V[i], w[i]).  \n",
    "knn_item_matrix = np.concatenate((V[nb_users:(nb_users+nb_movies)], w[nb_users:(nb_users+nb_movies)]), axis=1)\n",
    "knn_train_label = np.arange(1,nb_movies+1)\n",
    "\n",
    "#user latent matrix - concat (V[u], 1) \n",
    "ones = np.ones(nb_users).reshape((nb_users, 1))\n",
    "knn_user_matrix = np.concatenate((V[:nb_users], ones), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KNN train features shape = ', knn_item_matrix.shape)\n",
    "knn_prefix = 'knn'\n",
    "knn_output_prefix  = 's3://{}/{}/output'.format(bucket, knn_prefix)\n",
    "knn_train_data_path = writeDatasetToProtobuf(knn_item_matrix, bucket, knn_prefix, train_key, \"dense\", knn_train_label)\n",
    "print('uploaded KNN train data: {}'.format(knn_train_data_path))\n",
    "\n",
    "nb_recommendations = 100\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(framework='knn', region='us-east-1', version='latest')\n",
    "# crole='AmazonSageMaker-ExecutionRole-20200603T105247' \n",
    "crole = 'arn:aws:iam::230755935769:role/service-role/AmazonSageMaker-ExecutionRole-20200603T105247'\n",
    "instance_type='ml.m5.large'\n",
    "# crole = get_execution_role()\n",
    "\n",
    "# set up the estimator\n",
    "knn = sagemaker.estimator.Estimator(container,\n",
    "    crole,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    sagemaker_session=sagemaker.Session())\n",
    "\n",
    "knn.set_hyperparameters(feature_dim=knn_item_matrix.shape[1], k=nb_recommendations, index_metric=\"INNER_PRODUCT\", predictor_type='classifier', sample_size=200000)\n",
    "fit_input = {'train': knn_train_data_path}\n",
    "knn.fit(fit_input)\n",
    "knn_model_name =  knn.latest_training_job.job_name\n",
    "print(\"created model: \", knn_model_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model so that we can reference it in the next step during batch inference\n",
    "sm = boto3.client(service_name='sagemaker')\n",
    "\n",
    "print(knn.training_image_uri())\n",
    "primary_container = {\n",
    "    'Image': knn.training_image_uri(),\n",
    "    'ModelDataUrl': knn.model_data,\n",
    "}\n",
    "\n",
    "\n",
    "knn_model = sm.create_model(\n",
    "        ModelName = knn.latest_training_job.job_name,\n",
    "        ExecutionRoleArn = knn.role,\n",
    "        PrimaryContainer = primary_container)\n",
    "print(\"saved the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload inference data to S3\n",
    "knn_batch_data_path = writeDatasetToProtobuf(knn_user_matrix, bucket, knn_prefix, train_key, \"dense\")\n",
    "print(\"Batch inference data path: \",knn_batch_data_path) \n",
    "\n",
    "# Initialize the transformer object\n",
    "knn_transformer =sagemaker.transformer.Transformer(\n",
    "    base_transform_job_name=\"knn\",\n",
    "    model_name=knn_model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=knn_output_prefix,\n",
    "    accept=\"application/jsonlines; verbose=true\"\n",
    ")\n",
    "\n",
    "# Start a transform job:\n",
    "knn_transformer.transform(knn_batch_data_path, content_type='application/x-recordio-protobuf')\n",
    "knn_transformer.wait()\n",
    "\n",
    "\n",
    "#Download predictions \n",
    "results_file_name = \"inference_output\"\n",
    "inference_output_file = \"knn/output/train.protobuf.out\"\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.download_file(bucket, inference_output_file, results_file_name)\n",
    "with open(results_file_name) as f:\n",
    "    results = f.readlines()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "user_inv_idx = {} \n",
    "item_inv_idx = {} \n",
    "for k, v in transformer.u_idx.items():\n",
    "    user_inv_idx[v] = k \n",
    "    \n",
    "for k, v in transformer.i_idx.items():\n",
    "    item_inv_idx[v] = k \n",
    "    \n",
    "test_user_idx = 89\n",
    "# test_user_idx = 200\n",
    "u_id = user_inv_idx[test_user_idx]\n",
    "\n",
    "tester_df = user_item_df[user_item_df['uid']==int(u_id)]\n",
    "tester_df['positive'] = tester_df['rating'] >3 \n",
    "\n",
    "review = tester_df[['positive']+genres].groupby(['positive']).sum()\n",
    "plot_heat_map(review, figsize=(10,5))\n",
    "\n",
    "\n",
    "test_positive_df = user_item_df[(user_item_df['uid']==int(u_id)) & (user_item_df['rating']>3)][['title']]\n",
    "print(test_positive_df[['title']])\n",
    "test_negative_df = user_item_df[(user_item_df['uid']==int(u_id)) & (user_item_df['rating']<3)][['title']]\n",
    "print(test_negative_df[['title']])\n",
    "\n",
    "u_one_json = json.loads(results[test_user_idx])\n",
    "    \n",
    "    \n",
    "\n",
    "print (\"Recommended movie Ids for user #{} : {}\".format(test_user_idx+1, [item_inv_idx[movie_id] for movie_id in u_one_json['labels']]))\n",
    "\n",
    "for movie_id in u_one_json['labels']:\n",
    "    i_id = item_inv_idx[movie_id]\n",
    "    print (item_df[item_df['iid']==int(i_id)]['title'])\n",
    "\n",
    "print (\"Movie distances for user #{} : {}\".format(test_user_idx+1,  [round(distance, 4) for distance in u_one_json['distances']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, Y_test,X_cold_test, Y_cold_test\n",
    "from sagemaker_utils.query_serializer import SparseFormatSerializer \n",
    "# from sklearn.metrics import mean_squared_error\n",
    "import numpy \n",
    "\n",
    "sparse = SparseFormatSerializer(nFeatures)\n",
    "fm_predictor.serializer = sparse\n",
    "fm_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "def model_rmse(X_test, Y_test): \n",
    "    X_test_arr = X_test\n",
    "        \n",
    "    result = fm_predictor.predict(X_test_arr) \n",
    "    y_pred = [] \n",
    "    for p in result['predictions']: \n",
    "        y_pred.append(p['score'])\n",
    "    \n",
    "    return numpy.sqrt(numpy.mean((y_pred-Y_test)**2))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def model_accuracy(X_test, Y_test): \n",
    "    X_test_arr = X_test\n",
    "        \n",
    "    result = fm_predictor.predict(X_test_arr) \n",
    "    print(result)\n",
    "    y_pred = [] \n",
    "    for p in result['predictions']: \n",
    "        if p['score'] > 0.5:\n",
    "            y_pred.append(1)\n",
    "        else: \n",
    "            y_pred.append(0)\n",
    "    return accuracy_score(Y_test, y_pred, normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = model_accuracy(X_train[:3000], Y_train[:3000]) / len(Y_train[:3000])\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = model_accuracy(X_test, Y_test) / len(Y_test)\n",
    "rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = model_accuracy(X_cold_test, Y_cold_test) / len(Y_cold_test)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = item_df[['iid']+genres].set_index('iid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = user_item_df[user_item_df['rating']>3].groupby('iid').count()['uid'].to_dict() \n",
    "answer = {} \n",
    "for ui in test_user_item: \n",
    "    uid = ui[0]\n",
    "    iid = ui[1]\n",
    "    rating = ui[2]\n",
    "    if uid not in answer:  \n",
    "        answer[uid] = set() \n",
    "    if rating > 3: \n",
    "        answer[uid].add(iid)\n",
    "    \n",
    "all_pop = 0 \n",
    "hits = 0 \n",
    "topk = 10\n",
    "valid_user_nb = 0 \n",
    "rcmded = set()\n",
    "for uid in answer.keys():\n",
    "    if uid in transformer.u_idx: \n",
    "        cuidx = transformer.u_idx[uid] \n",
    "        u_one_json = json.loads(results[cuidx])\n",
    "        pred = set()\n",
    "        valid_user_nb += 1 \n",
    "        for i, movieid in enumerate(u_one_json['labels'][:topk]): \n",
    "            rcmded.add(movieid)\n",
    "            if int(item_inv_idx[movieid]) in popularity: \n",
    "                all_pop += popularity[int(item_inv_idx[movieid])] \n",
    "            pred.add(item_inv_idx[movieid])\n",
    "        hits += len(pred.intersection(answer[uid]))\n",
    "print(all_pop/topk/valid_user_nb)\n",
    "print(len(answer))\n",
    "print(hits)\n",
    "print(len(rcmded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
