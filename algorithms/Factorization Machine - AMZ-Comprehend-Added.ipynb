{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-08-10 23:24:38--  https://tinyurl.com/y3gfnlx2\n",
      "Resolving tinyurl.com (tinyurl.com)... 104.20.138.65, 104.20.139.65, 172.67.1.225\n",
      "Connecting to tinyurl.com (tinyurl.com)|104.20.138.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://recommendation-demo-yianc.s3.us-east-1.amazonaws.com/amz-review/amz-review-apparel.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIATLORAEYMTX7JY4ER%2F20200810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200810T152347Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=e6c35dc78526ddb7cd7e913fa63c5f8a5ad57a39ca556ab68fe010ed58051435 [following]\n",
      "--2020-08-10 23:24:39--  https://recommendation-demo-yianc.s3.us-east-1.amazonaws.com/amz-review/amz-review-apparel.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIATLORAEYMTX7JY4ER%2F20200810%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200810T152347Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=e6c35dc78526ddb7cd7e913fa63c5f8a5ad57a39ca556ab68fe010ed58051435\n",
      "Resolving recommendation-demo-yianc.s3.us-east-1.amazonaws.com (recommendation-demo-yianc.s3.us-east-1.amazonaws.com)... 52.216.97.142\n",
      "Connecting to recommendation-demo-yianc.s3.us-east-1.amazonaws.com (recommendation-demo-yianc.s3.us-east-1.amazonaws.com)|52.216.97.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 621522774 (593M) [text/csv]\n",
      "Saving to: ‘amz-review-apparel.csv’\n",
      "\n",
      "amz-review-apparel. 100%[===================>] 592.73M  1.43MB/s    in 7m 18s  \n",
      "\n",
      "2020-08-10 23:31:57 (1.35 MB/s) - ‘amz-review-apparel.csv’ saved [621522774/621522774]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O amz-review-apparel.csv https://tinyurl.com/y3gfnlx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>cnt</th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id.1</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28563435</td>\n",
       "      <td>13</td>\n",
       "      <td>US</td>\n",
       "      <td>28563435</td>\n",
       "      <td>R2M9YYZ4DLZRMN</td>\n",
       "      <td>B003OQTQ0W</td>\n",
       "      <td>97286984</td>\n",
       "      <td>Carhartt Men's Two-Tone Trifold Wallet</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Outstanding Wallet</td>\n",
       "      <td>I have owned a lot of wallets over the years a...</td>\n",
       "      <td>2013-07-11</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28563435</td>\n",
       "      <td>13</td>\n",
       "      <td>US</td>\n",
       "      <td>28563435</td>\n",
       "      <td>R2DIHSYWNP3FPJ</td>\n",
       "      <td>B0093O17U6</td>\n",
       "      <td>765210786</td>\n",
       "      <td>IH Camouflage Trucker Cap in Mossy Oak Break-U...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great cap</td>\n",
       "      <td>This is a very nice hat,but i have got another...</td>\n",
       "      <td>2013-05-11</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28563435</td>\n",
       "      <td>13</td>\n",
       "      <td>US</td>\n",
       "      <td>28563435</td>\n",
       "      <td>R36XY86RBEESP6</td>\n",
       "      <td>B0051I6MYY</td>\n",
       "      <td>268391293</td>\n",
       "      <td>IH 5 Panel Trucker Cap with Liquid Metal Logo</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great hat!!!</td>\n",
       "      <td>This is a great hat and is very well made!! I ...</td>\n",
       "      <td>2013-04-13</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28563435</td>\n",
       "      <td>13</td>\n",
       "      <td>US</td>\n",
       "      <td>28563435</td>\n",
       "      <td>R2X9GLV67VACAN</td>\n",
       "      <td>B005F29FKO</td>\n",
       "      <td>506334102</td>\n",
       "      <td>Carhartt Men's Relaxed Straight Denim Five Poc...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great pants</td>\n",
       "      <td>I have always been a Wrangler man most of my l...</td>\n",
       "      <td>2012-12-08</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28563435</td>\n",
       "      <td>13</td>\n",
       "      <td>US</td>\n",
       "      <td>28563435</td>\n",
       "      <td>RS4RG84BX2KK5</td>\n",
       "      <td>B004QF0THY</td>\n",
       "      <td>206343191</td>\n",
       "      <td>Dickies Men's 2 Pack Wool Blend Boot Crew Socks</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Very Great socks!!</td>\n",
       "      <td>Wool is the only socks i wear and the only thi...</td>\n",
       "      <td>2012-12-08</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  cnt marketplace  customer_id.1       review_id  product_id  \\\n",
       "0     28563435   13          US       28563435  R2M9YYZ4DLZRMN  B003OQTQ0W   \n",
       "1     28563435   13          US       28563435  R2DIHSYWNP3FPJ  B0093O17U6   \n",
       "2     28563435   13          US       28563435  R36XY86RBEESP6  B0051I6MYY   \n",
       "3     28563435   13          US       28563435  R2X9GLV67VACAN  B005F29FKO   \n",
       "4     28563435   13          US       28563435   RS4RG84BX2KK5  B004QF0THY   \n",
       "\n",
       "   product_parent                                      product_title  \\\n",
       "0        97286984             Carhartt Men's Two-Tone Trifold Wallet   \n",
       "1       765210786  IH Camouflage Trucker Cap in Mossy Oak Break-U...   \n",
       "2       268391293      IH 5 Panel Trucker Cap with Liquid Metal Logo   \n",
       "3       506334102  Carhartt Men's Relaxed Straight Denim Five Poc...   \n",
       "4       206343191    Dickies Men's 2 Pack Wool Blend Boot Crew Socks   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            5              0            0    N                 Y   \n",
       "1            5              0            0    N                 Y   \n",
       "2            5              1            1    N                 Y   \n",
       "3            5              0            0    N                 Y   \n",
       "4            5              0            0    N                 Y   \n",
       "\n",
       "      review_headline                                        review_body  \\\n",
       "0  Outstanding Wallet  I have owned a lot of wallets over the years a...   \n",
       "1           Great cap  This is a very nice hat,but i have got another...   \n",
       "2        Great hat!!!  This is a great hat and is very well made!! I ...   \n",
       "3         Great pants  I have always been a Wrangler man most of my l...   \n",
       "4  Very Great socks!!  Wool is the only socks i wear and the only thi...   \n",
       "\n",
       "  review_date  year  \n",
       "0  2013-07-11  2013  \n",
       "1  2013-05-11  2013  \n",
       "2  2013-04-13  2013  \n",
       "3  2012-12-08  2012  \n",
       "4  2012-12-08  2012  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "user_item = './amz-review-apparel.csv'\n",
    "\n",
    "user_item_df = pd.read_csv(user_item)\n",
    "user_item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from preprocessing.amz_datareader import AMZComprehendDataReader\n",
    "from preprocessing.factorization_machine_transformer import  FactorizationMachineTransformer\n",
    "\n",
    "\n",
    "user_item = './amz-review-apparel.csv'\n",
    "comprehend_item_data = './item-topics.csv'\n",
    "reader = AMZComprehendDataReader()\n",
    "user_item  = reader.read_user_item_rating(user_item)\n",
    "users = {}\n",
    "items = reader.read_item_data(comprehend_item_data)\n",
    "train_user_item = user_item[:int(len(user_item)*0.8)]\n",
    "test_user_item = user_item[int(len(user_item)*0.8):]\n",
    "transformer = FactorizationMachineTransformer(users, items, train_user_item)\n",
    "X_train, Y_train, nFeatures, processed = transformer.get_feature_vectors(users, items, train_user_item)\n",
    "X_test, Y_test, nFeatures, processed = transformer.get_feature_vectors(users, items, test_user_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bucket! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BytesIO object at 0x11aa21d10>\n",
      "Output: s3://recommendation-demo-yianc/sagemaker/fm-amz-comprehend/output\n"
     ]
    }
   ],
   "source": [
    "bucket = 'recommendation-demo-yianc'\n",
    "prefix = 'sagemaker/fm-amz-comprehend'\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}/'.format(prefix, 'test')\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    import io,boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    print(buf)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    " \n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key)    \n",
    "  \n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://recommendation-demo-yianc/sagemaker/fm-amz-comprehend/train/train.protobuf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker \n",
    "\n",
    "container = sagemaker.image_uris.retrieve(framework='factorization-machines', region='us-east-1', version='latest')\n",
    "container \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sagemaker.estimator.Estimator'>\n",
      "2020-08-11 09:29:52 Starting - Starting the training job...\n",
      "2020-08-11 09:29:54 Starting - Launching requested ML instances......\n",
      "2020-08-11 09:31:11 Starting - Preparing the instances for training......\n",
      "2020-08-11 09:32:21 Downloading - Downloading input data...\n",
      "2020-08-11 09:33:17 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 INFO 139921924822848] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-conf.json: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': 1, u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 INFO 139921924822848] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'epochs': u'50', u'feature_dim': u'980914', u'mini_batch_size': u'1000', u'predictor_type': u'regressor', u'num_factors': u'64'}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 INFO 139921924822848] Final configuration: {u'factors_lr': u'0.0001', u'linear_init_sigma': u'0.01', u'epochs': u'50', u'feature_dim': u'980914', u'num_factors': u'64', u'_wd': u'1.0', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'factors_init_sigma': u'0.001', u'_log_level': u'info', u'bias_init_method': u'normal', u'linear_init_method': u'normal', u'linear_lr': u'0.001', u'factors_init_method': u'normal', u'_tuning_objective_metric': u'', u'bias_wd': u'0.01', u'use_linear': u'true', u'bias_lr': u'0.1', u'mini_batch_size': u'1000', u'_use_full_symbolic': u'true', u'batch_metrics_publish_interval': u'500', u'predictor_type': u'regressor', u'bias_init_sigma': u'0.01', u'_num_gpus': u'auto', u'_data_format': u'record', u'factors_wd': u'0.00001', u'linear_wd': u'0.001', u'_kvstore': u'auto', u'_learning_rate': u'1.0', u'_optimizer': u'adam'}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 WARNING 139921924822848] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 INFO 139921924822848] Using default worker.\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:33:21.128] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 6, \"num_examples\": 1, \"num_bytes\": 71888}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 INFO 139921924822848] nvidia-smi took: 0.0252718925476 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 INFO 139921924822848] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 INFO 139921924822848] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:21 INFO 139921924822848] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 36.160945892333984, \"sum\": 36.160945892333984, \"min\": 36.160945892333984}}, \"EndTime\": 1597138401.162305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138401.120872}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"Total Records Seen\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1000, \"sum\": 1000.0, \"min\": 1000}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1597138401.1625, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138401.162455}\n",
      "\u001b[0m\n",
      "\u001b[34m[09:33:21] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.203343.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[09:33:21] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.203343.0/AL2012/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=4.35743191463\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=18.9872128906\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=4.17503027344\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:32 INFO 139921924822848] Iter[0] Batch [500]#011Speed: 55863.89 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=500 train rmse <loss>=1.36024601321\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=500 train mse <loss>=1.85026921645\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=500 train absolute_loss <loss>=1.0535736084\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:41 INFO 139921924822848] Iter[0] Batch [1000]#011Speed: 59300.21 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=1000 train rmse <loss>=1.24943387088\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=1000 train mse <loss>=1.5610849977\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, batch=1000 train absolute_loss <loss>=0.979793814096\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:33:46.220] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 23232, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.22904897216\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, train mse <loss>=1.51056137597\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=0.967156157508\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}, \"update.time\": {\"count\": 1, \"max\": 25057.968854904175, \"sum\": 25057.968854904175, \"min\": 25057.968854904175}}, \"EndTime\": 1597138426.220681, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138401.162381}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:46 INFO 139921924822848] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1301, \"sum\": 1301.0, \"min\": 1301}, \"Total Records Seen\": {\"count\": 1, \"max\": 1300471, \"sum\": 1300471.0, \"min\": 1300471}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1597138426.220922, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 0}, \"StartTime\": 1597138401.16268}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:46 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=51857.7938393 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.24157713975\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.54151379395\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.99347668457\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:33:54 INFO 139921924822848] Iter[1] Batch [500]#011Speed: 58168.49 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=500 train rmse <loss>=1.13735839851\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=500 train mse <loss>=1.29358412667\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:33:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=500 train absolute_loss <loss>=0.911297512884\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:03 INFO 139921924822848] Iter[1] Batch [1000]#011Speed: 59395.46 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=1000 train rmse <loss>=1.12018779321\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=1000 train mse <loss>=1.25482069206\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, batch=1000 train absolute_loss <loss>=0.895394542103\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:34:08.349] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 22125, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.12209415747\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.25909529823\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.895237836538\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22127.749919891357, \"sum\": 22127.749919891357, \"min\": 22127.749919891357}}, \"EndTime\": 1597138448.349913, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138426.220763}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:08 INFO 139921924822848] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2601, \"sum\": 2601.0, \"min\": 2601}, \"Total Records Seen\": {\"count\": 1, \"max\": 2599942, \"sum\": 2599942.0, \"min\": 2599942}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1597138448.350106, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 1}, \"StartTime\": 1597138426.222131}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:08 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58724.9164085 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.21557127532\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.47761352539\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.97339251709\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:16 INFO 139921924822848] Iter[2] Batch [500]#011Speed: 59169.26 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=500 train rmse <loss>=1.09834495004\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=500 train mse <loss>=1.20636162928\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=500 train absolute_loss <loss>=0.872758848879\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:25 INFO 139921924822848] Iter[2] Batch [1000]#011Speed: 59810.85 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=1000 train rmse <loss>=1.07934136854\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=1000 train mse <loss>=1.16497778985\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, batch=1000 train absolute_loss <loss>=0.853658505618\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:34:30.237] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 21885, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.08031268042\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.16707548748\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.852569789663\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21887.25709915161, \"sum\": 21887.25709915161, \"min\": 21887.25709915161}}, \"EndTime\": 1597138470.238677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138448.349987}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:30 INFO 139921924822848] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3901, \"sum\": 3901.0, \"min\": 3901}, \"Total Records Seen\": {\"count\": 1, \"max\": 3899413, \"sum\": 3899413.0, \"min\": 3899413}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1597138470.238884, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 2}, \"StartTime\": 1597138448.351391}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:30 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59370.183083 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=1.17173394984\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=1.37296044922\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.93294342041\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:38 INFO 139921924822848] Iter[3] Batch [500]#011Speed: 59211.36 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=500 train rmse <loss>=1.05175254493\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=500 train mse <loss>=1.10618341576\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=500 train absolute_loss <loss>=0.827509435231\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:47 INFO 139921924822848] Iter[3] Batch [1000]#011Speed: 59428.00 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=1000 train rmse <loss>=1.03323242198\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=1000 train mse <loss>=1.06756923783\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, batch=1000 train absolute_loss <loss>=0.80763659668\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:34:52.114] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 21874, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.03410686725\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.06937701289\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.806363578914\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21876.534938812256, \"sum\": 21876.534938812256, \"min\": 21876.534938812256}}, \"EndTime\": 1597138492.115681, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138470.238753}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:52 INFO 139921924822848] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5201, \"sum\": 5201.0, \"min\": 5201}, \"Total Records Seen\": {\"count\": 1, \"max\": 5198884, \"sum\": 5198884.0, \"min\": 5198884}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1597138492.115902, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 3}, \"StartTime\": 1597138470.239115}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:52 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59399.1750622 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=1.12510459583\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=1.26586035156\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:34:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.887309082031\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:35:00 INFO 139921924822848] Iter[4] Batch [500]#011Speed: 57968.02 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=500 train rmse <loss>=1.00913939797\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=500 train mse <loss>=1.01836232455\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=500 train absolute_loss <loss>=0.785506891978\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:09 INFO 139921924822848] Iter[4] Batch [1000]#011Speed: 58927.54 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=1000 train rmse <loss>=0.991255786874\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=1000 train mse <loss>=0.982588035012\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, batch=1000 train absolute_loss <loss>=0.765614269337\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:35:14.267] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 22148, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=0.991951291159\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, train mse <loss>=0.983967364032\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.764206888803\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22150.554895401, \"sum\": 22150.554895401, \"min\": 22150.554895401}}, \"EndTime\": 1597138514.26776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138492.115752}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:14 INFO 139921924822848] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6501, \"sum\": 6501.0, \"min\": 6501}, \"Total Records Seen\": {\"count\": 1, \"max\": 6498355, \"sum\": 6498355.0, \"min\": 6498355}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1597138514.268005, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 4}, \"StartTime\": 1597138492.117175}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:14 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58664.3753892 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=1.08230242541\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=1.17137854004\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.84137286377\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:22 INFO 139921924822848] Iter[5] Batch [500]#011Speed: 58896.17 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=500 train rmse <loss>=0.973106491649\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=500 train mse <loss>=0.946936244089\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=500 train absolute_loss <loss>=0.749226094564\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:31 INFO 139921924822848] Iter[5] Batch [1000]#011Speed: 59027.08 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=1000 train rmse <loss>=0.955280347555\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=1000 train mse <loss>=0.912560542424\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, batch=1000 train absolute_loss <loss>=0.729053080221\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:35:36.278] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 22008, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=0.955514719405\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, train mse <loss>=0.913008379\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.727316773729\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22010.823011398315, \"sum\": 22010.823011398315, \"min\": 22010.823011398315}}, \"EndTime\": 1597138536.279152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138514.267842}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:36 INFO 139921924822848] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7801, \"sum\": 7801.0, \"min\": 7801}, \"Total Records Seen\": {\"count\": 1, \"max\": 7797826, \"sum\": 7797826.0, \"min\": 7797826}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1597138536.279391, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 5}, \"StartTime\": 1597138514.268301}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:36 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59036.6910028 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=1.0448159555\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=1.09164038086\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=0.800012023926\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:44 INFO 139921924822848] Iter[6] Batch [500]#011Speed: 58068.52 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=500 train rmse <loss>=0.942604798217\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=500 train mse <loss>=0.888503805621\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=500 train absolute_loss <loss>=0.718245471703\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:53 INFO 139921924822848] Iter[6] Batch [1000]#011Speed: 59909.71 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=1000 train rmse <loss>=0.924376060797\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=1000 train mse <loss>=0.854471101774\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, batch=1000 train absolute_loss <loss>=0.697441060343\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:35:58.345] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 22064, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=0.923965961115\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, train mse <loss>=0.853713097299\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.695220477342\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22066.169023513794, \"sum\": 22066.169023513794, \"min\": 22066.169023513794}}, \"EndTime\": 1597138558.345867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138536.279231}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:58 INFO 139921924822848] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9101, \"sum\": 9101.0, \"min\": 9101}, \"Total Records Seen\": {\"count\": 1, \"max\": 9097297, \"sum\": 9097297.0, \"min\": 9097297}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1597138558.346101, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 6}, \"StartTime\": 1597138536.279665}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:58 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58888.7118372 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=1.0128339052\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=1.02583251953\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:35:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.765088745117\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:36:07 INFO 139921924822848] Iter[7] Batch [500]#011Speed: 57674.56 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=500 train rmse <loss>=0.916109942726\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=500 train mse <loss>=0.839257427162\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=500 train absolute_loss <loss>=0.691474753009\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:15 INFO 139921924822848] Iter[7] Batch [1000]#011Speed: 59568.07 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=1000 train rmse <loss>=0.89725606063\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=1000 train mse <loss>=0.805068438337\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, batch=1000 train absolute_loss <loss>=0.669856390863\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:36:20.418] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 22070, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=0.896129186592\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, train mse <loss>=0.803047519062\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.667069194946\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22072.613954544067, \"sum\": 22072.613954544067, \"min\": 22072.613954544067}}, \"EndTime\": 1597138580.419014, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138558.345937}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:20 INFO 139921924822848] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10401, \"sum\": 10401.0, \"min\": 10401}, \"Total Records Seen\": {\"count\": 1, \"max\": 10396768, \"sum\": 10396768.0, \"min\": 10396768}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 9, \"sum\": 9.0, \"min\": 9}}, \"EndTime\": 1597138580.419209, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 7}, \"StartTime\": 1597138558.346373}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:20 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58871.6750622 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=0.985826558734\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=0.971854003906\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.73603729248\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:29 INFO 139921924822848] Iter[8] Batch [500]#011Speed: 58245.91 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=500 train rmse <loss>=0.892458829857\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=500 train mse <loss>=0.79648276299\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=500 train absolute_loss <loss>=0.667795311233\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:37 INFO 139921924822848] Iter[8] Batch [1000]#011Speed: 59400.62 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=1000 train rmse <loss>=0.872927457911\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=1000 train mse <loss>=0.762002346774\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, batch=1000 train absolute_loss <loss>=0.645416585734\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:36:42.490] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 22069, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=0.871082493325\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, train mse <loss>=0.758784710177\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.642056805044\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22071.549892425537, \"sum\": 22071.549892425537, \"min\": 22071.549892425537}}, \"EndTime\": 1597138602.491012, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138580.419082}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:42 INFO 139921924822848] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11701, \"sum\": 11701.0, \"min\": 11701}, \"Total Records Seen\": {\"count\": 1, \"max\": 11696239, \"sum\": 11696239.0, \"min\": 11696239}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1597138602.491206, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 8}, \"StartTime\": 1597138580.419432}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:42 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58874.4909456 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=0.962857140011\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=0.92709387207\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.711025146484\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:51 INFO 139921924822848] Iter[9] Batch [500]#011Speed: 58879.83 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=500 train rmse <loss>=0.870889599577\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=500 train mse <loss>=0.758448694652\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=500 train absolute_loss <loss>=0.646430475658\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:59 INFO 139921924822848] Iter[9] Batch [1000]#011Speed: 59272.01 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=1000 train rmse <loss>=0.850722719455\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=1000 train mse <loss>=0.723729145397\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:36:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, batch=1000 train absolute_loss <loss>=0.623416041326\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:37:04.521] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 22027, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=0.848196545547\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, train mse <loss>=0.719437379878\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.619514694801\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22029.42395210266, \"sum\": 22029.42395210266, \"min\": 22029.42395210266}}, \"EndTime\": 1597138624.521926, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138602.491076}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:04 INFO 139921924822848] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 13001, \"sum\": 13001.0, \"min\": 13001}, \"Total Records Seen\": {\"count\": 1, \"max\": 12995710, \"sum\": 12995710.0, \"min\": 12995710}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 11, \"sum\": 11.0, \"min\": 11}}, \"EndTime\": 1597138624.522125, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 9}, \"StartTime\": 1597138602.492474}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:04 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58987.0749974 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=0.94297133906\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=0.889194946289\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=0.69095098877\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:13 INFO 139921924822848] Iter[10] Batch [500]#011Speed: 58120.36 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=500 train rmse <loss>=0.850926133965\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=500 train mse <loss>=0.724075285464\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=500 train absolute_loss <loss>=0.626922483707\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:37:21 INFO 139921924822848] Iter[10] Batch [1000]#011Speed: 59566.34 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=1000 train rmse <loss>=0.830209691122\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=1000 train mse <loss>=0.689248131233\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, batch=1000 train absolute_loss <loss>=0.6034188719\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:37:26.599] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 22075, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=0.827056928402\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, train mse <loss>=0.684023162818\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=0.599038349891\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22077.817916870117, \"sum\": 22077.817916870117, \"min\": 22077.817916870117}}, \"EndTime\": 1597138646.600217, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138624.521995}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:26 INFO 139921924822848] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14301, \"sum\": 14301.0, \"min\": 14301}, \"Total Records Seen\": {\"count\": 1, \"max\": 14295181, \"sum\": 14295181.0, \"min\": 14295181}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1597138646.600409, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 10}, \"StartTime\": 1597138624.52237}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:26 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58857.7712948 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=0.925386024551\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=0.856339294434\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=0.674426086426\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:35 INFO 139921924822848] Iter[11] Batch [500]#011Speed: 58567.59 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=500 train rmse <loss>=0.832271322006\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=500 train mse <loss>=0.692675553434\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=500 train absolute_loss <loss>=0.608907169852\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:43 INFO 139921924822848] Iter[11] Batch [1000]#011Speed: 59803.23 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:43 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=1000 train rmse <loss>=0.811106415338\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:43 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=1000 train mse <loss>=0.657893617003\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:43 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, batch=1000 train absolute_loss <loss>=0.585051632541\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:37:48.508] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 21905, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=0.807387430302\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, train mse <loss>=0.651874462609\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=0.580242197031\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21907.58490562439, \"sum\": 21907.58490562439, \"min\": 21907.58490562439}}, \"EndTime\": 1597138668.509326, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138646.600278}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:48 INFO 139921924822848] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 15601, \"sum\": 15601.0, \"min\": 15601}, \"Total Records Seen\": {\"count\": 1, \"max\": 15594652, \"sum\": 15594652.0, \"min\": 15594652}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 13, \"sum\": 13.0, \"min\": 13}}, \"EndTime\": 1597138668.509572, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 11}, \"StartTime\": 1597138646.60171}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:48 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59314.9781773 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=0.909528317456\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=0.827241760254\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=0.659974060059\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:57 INFO 139921924822848] Iter[12] Batch [500]#011Speed: 59055.47 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=500 train rmse <loss>=0.814733445316\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=500 train mse <loss>=0.663790586917\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:37:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=500 train absolute_loss <loss>=0.592105595084\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:05 INFO 139921924822848] Iter[12] Batch [1000]#011Speed: 59631.45 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:05 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=1000 train rmse <loss>=0.793221119742\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:05 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=1000 train mse <loss>=0.629199744805\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:05 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, batch=1000 train absolute_loss <loss>=0.568032279945\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:38:10.413] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 21902, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=0.788995924056\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, train mse <loss>=0.622514568176\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=0.562848172913\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21904.245138168335, \"sum\": 21904.245138168335, \"min\": 21904.245138168335}}, \"EndTime\": 1597138690.414117, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138668.509401}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:10 INFO 139921924822848] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16901, \"sum\": 16901.0, \"min\": 16901}, \"Total Records Seen\": {\"count\": 1, \"max\": 16894123, \"sum\": 16894123.0, \"min\": 16894123}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1597138690.414351, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 12}, \"StartTime\": 1597138668.509842}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:10 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59324.0360505 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=0.894997592688\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=0.801020690918\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=0.64707244873\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:38:18 INFO 139921924822848] Iter[13] Batch [500]#011Speed: 59310.30 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:18 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=500 train rmse <loss>=0.798182252424\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:18 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=500 train mse <loss>=0.637094908084\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:18 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=500 train absolute_loss <loss>=0.576398583631\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:27 INFO 139921924822848] Iter[13] Batch [1000]#011Speed: 59689.52 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:27 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=1000 train rmse <loss>=0.776415934857\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:27 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=1000 train mse <loss>=0.6028217039\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:27 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, batch=1000 train absolute_loss <loss>=0.552254803363\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:38:32.273] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 21856, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=0.771741454003\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, train mse <loss>=0.595584871826\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=0.546768863572\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21858.95800590515, \"sum\": 21858.95800590515, \"min\": 21858.95800590515}}, \"EndTime\": 1597138712.273563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138690.414178}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:32 INFO 139921924822848] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18201, \"sum\": 18201.0, \"min\": 18201}, \"Total Records Seen\": {\"count\": 1, \"max\": 18193594, \"sum\": 18193594.0, \"min\": 18193594}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 15, \"sum\": 15.0, \"min\": 15}}, \"EndTime\": 1597138712.273792, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 13}, \"StartTime\": 1597138690.414575}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:32 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59446.9926037 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=0.881504285121\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=0.777049804687\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=0.635422546387\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:40 INFO 139921924822848] Iter[14] Batch [500]#011Speed: 59463.59 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:40 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=500 train rmse <loss>=0.78252424211\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:40 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=500 train mse <loss>=0.61234418949\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:40 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=500 train absolute_loss <loss>=0.561743415025\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:49 INFO 139921924822848] Iter[14] Batch [1000]#011Speed: 59366.99 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:49 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=1000 train rmse <loss>=0.760586822005\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:49 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=1000 train mse <loss>=0.578492313808\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:49 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, batch=1000 train absolute_loss <loss>=0.537666055106\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:38:54.129] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 21853, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=0.755515568503\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, train mse <loss>=0.57080377425\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=0.531937313655\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21855.751037597656, \"sum\": 21855.751037597656, \"min\": 21855.751037597656}}, \"EndTime\": 1597138734.129785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138712.27365}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:54 INFO 139921924822848] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19501, \"sum\": 19501.0, \"min\": 19501}, \"Total Records Seen\": {\"count\": 1, \"max\": 19493065, \"sum\": 19493065.0, \"min\": 19493065}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1597138734.129992, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 14}, \"StartTime\": 1597138712.274006}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:54 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59455.7853627 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=0.868824679232\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=0.754856323242\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:38:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=0.625413085938\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:02 INFO 139921924822848] Iter[15] Batch [500]#011Speed: 58266.39 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:02 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=500 train rmse <loss>=0.767688283531\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:02 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=500 train mse <loss>=0.589345300671\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:02 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=500 train absolute_loss <loss>=0.548008032763\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:11 INFO 139921924822848] Iter[15] Batch [1000]#011Speed: 59260.60 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:11 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=1000 train rmse <loss>=0.745652089152\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:11 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=1000 train mse <loss>=0.555997038057\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:11 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, batch=1000 train absolute_loss <loss>=0.524089192241\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:39:16.193] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 22061, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=0.740231528958\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, train mse <loss>=0.547942716464\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=0.518154388076\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22063.349962234497, \"sum\": 22063.349962234497, \"min\": 22063.349962234497}}, \"EndTime\": 1597138756.193612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138734.129857}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:16 INFO 139921924822848] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20801, \"sum\": 20801.0, \"min\": 20801}, \"Total Records Seen\": {\"count\": 1, \"max\": 20792536, \"sum\": 20792536.0, \"min\": 20792536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 17, \"sum\": 17.0, \"min\": 17}}, \"EndTime\": 1597138756.193816, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 15}, \"StartTime\": 1597138734.130209}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:16 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58896.3053591 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=0.85677792946\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=0.73406842041\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=0.616337646484\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:39:24 INFO 139921924822848] Iter[16] Batch [500]#011Speed: 59100.63 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=500 train rmse <loss>=0.753617126445\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=500 train mse <loss>=0.567938773271\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=500 train absolute_loss <loss>=0.535101860001\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:33 INFO 139921924822848] Iter[16] Batch [1000]#011Speed: 58800.32 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:33 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=1000 train rmse <loss>=0.731545476702\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:33 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=1000 train mse <loss>=0.535158784483\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:33 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, batch=1000 train absolute_loss <loss>=0.511405719708\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:39:38.247] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 22050, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=0.72581800149\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, train mse <loss>=0.526811771287\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=0.505294824054\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22052.836179733276, \"sum\": 22052.836179733276, \"min\": 22052.836179733276}}, \"EndTime\": 1597138778.248057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138756.193683}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:38 INFO 139921924822848] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22101, \"sum\": 22101.0, \"min\": 22101}, \"Total Records Seen\": {\"count\": 1, \"max\": 22092007, \"sum\": 22092007.0, \"min\": 22092007}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1597138778.248284, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 16}, \"StartTime\": 1597138756.195189}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:38 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58924.2699331 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=0.845219026428\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=0.714395202637\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=0.607814697266\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:46 INFO 139921924822848] Iter[17] Batch [500]#011Speed: 58882.42 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=500 train rmse <loss>=0.74026267476\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=500 train mse <loss>=0.547988827643\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=500 train absolute_loss <loss>=0.522940226663\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:55 INFO 139921924822848] Iter[17] Batch [1000]#011Speed: 60093.94 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:55 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=1000 train rmse <loss>=0.718212302615\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:55 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=1000 train mse <loss>=0.515828911628\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:39:55 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, batch=1000 train absolute_loss <loss>=0.499513296487\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:40:00.164] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 21914, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=0.712215642751\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, train mse <loss>=0.507251121779\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=0.493247056204\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21916.812896728516, \"sum\": 21916.812896728516, \"min\": 21916.812896728516}}, \"EndTime\": 1597138800.165433, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138778.248129}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:00 INFO 139921924822848] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 23401, \"sum\": 23401.0, \"min\": 23401}, \"Total Records Seen\": {\"count\": 1, \"max\": 23391478, \"sum\": 23391478.0, \"min\": 23391478}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}}, \"EndTime\": 1597138800.165626, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 17}, \"StartTime\": 1597138778.248588}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:00 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59290.0771548 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=0.83403558464\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=0.695615356445\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=0.59927911377\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:08 INFO 139921924822848] Iter[18] Batch [500]#011Speed: 58101.40 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=500 train rmse <loss>=0.727583901628\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=500 train mse <loss>=0.529378333908\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=500 train absolute_loss <loss>=0.511440236714\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:17 INFO 139921924822848] Iter[18] Batch [1000]#011Speed: 59394.78 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=1000 train rmse <loss>=0.705608309256\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=1000 train mse <loss>=0.497883086091\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, batch=1000 train absolute_loss <loss>=0.488326246782\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:40:22.291] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 22123, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=0.699376199321\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, train mse <loss>=0.489127068176\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=0.481927224356\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22126.08289718628, \"sum\": 22126.08289718628, \"min\": 22126.08289718628}}, \"EndTime\": 1597138822.291956, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138800.165497}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:22 INFO 139921924822848] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24701, \"sum\": 24701.0, \"min\": 24701}, \"Total Records Seen\": {\"count\": 1, \"max\": 24690949, \"sum\": 24690949.0, \"min\": 24690949}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1597138822.292191, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 18}, \"StartTime\": 1597138800.16584}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:22 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58729.250295 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=0.823146851872\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=0.677570739746\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=0.590548522949\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:40:30 INFO 139921924822848] Iter[19] Batch [500]#011Speed: 58664.54 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=500 train rmse <loss>=0.715547158384\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=500 train mse <loss>=0.512007735871\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=500 train absolute_loss <loss>=0.500550818285\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:39 INFO 139921924822848] Iter[19] Batch [1000]#011Speed: 59241.89 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=1000 train rmse <loss>=0.693702452395\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=1000 train mse <loss>=0.481223092459\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, batch=1000 train absolute_loss <loss>=0.477789997112\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:40:44.296] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 22001, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=0.687265418127\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, train mse <loss>=0.472333754953\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=0.471283694904\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22004.194021224976, \"sum\": 22004.194021224976, \"min\": 22004.194021224976}}, \"EndTime\": 1597138844.296669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138822.292034}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:44 INFO 139921924822848] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 26001, \"sum\": 26001.0, \"min\": 26001}, \"Total Records Seen\": {\"count\": 1, \"max\": 25990420, \"sum\": 25990420.0, \"min\": 25990420}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 21, \"sum\": 21.0, \"min\": 21}}, \"EndTime\": 1597138844.296868, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 19}, \"StartTime\": 1597138822.292445}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:44 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59054.7064216 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=0 train rmse <loss>=0.81249598106\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=0 train mse <loss>=0.660149719238\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=0 train absolute_loss <loss>=0.581570068359\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:52 INFO 139921924822848] Iter[20] Batch [500]#011Speed: 58929.14 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=500 train rmse <loss>=0.704130234493\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=500 train mse <loss>=0.495799387127\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:40:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=500 train absolute_loss <loss>=0.490237888664\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:01 INFO 139921924822848] Iter[20] Batch [1000]#011Speed: 59167.67 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=1000 train rmse <loss>=0.682489430341\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=1000 train mse <loss>=0.465791822527\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, batch=1000 train absolute_loss <loss>=0.467906498653\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:41:06.369] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 42, \"duration\": 22070, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, train rmse <loss>=0.675876373155\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, train mse <loss>=0.456808871789\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=20, train absolute_loss <loss>=0.461321867465\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22073.01688194275, \"sum\": 22073.01688194275, \"min\": 22073.01688194275}}, \"EndTime\": 1597138866.37013, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138844.296738}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:06 INFO 139921924822848] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 27301, \"sum\": 27301.0, \"min\": 27301}, \"Total Records Seen\": {\"count\": 1, \"max\": 27289891, \"sum\": 27289891.0, \"min\": 27289891}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1597138866.370408, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 20}, \"StartTime\": 1597138844.297077}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:06 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58870.2691291 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=0 train rmse <loss>=0.802004172906\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=0 train mse <loss>=0.643210693359\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=0 train absolute_loss <loss>=0.572500793457\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:15 INFO 139921924822848] Iter[21] Batch [500]#011Speed: 58076.16 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=500 train rmse <loss>=0.693338826224\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=500 train mse <loss>=0.480718727949\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=500 train absolute_loss <loss>=0.480515256109\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:23 INFO 139921924822848] Iter[21] Batch [1000]#011Speed: 58939.65 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=1000 train rmse <loss>=0.672043965404\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=1000 train mse <loss>=0.451643091436\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, batch=1000 train absolute_loss <loss>=0.458831540072\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:41:28.530] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 22157, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, train rmse <loss>=0.665297097836\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, train mse <loss>=0.442620228389\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=21, train absolute_loss <loss>=0.452252375793\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22159.42907333374, \"sum\": 22159.42907333374, \"min\": 22159.42907333374}}, \"EndTime\": 1597138888.53131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138866.370207}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:28 INFO 139921924822848] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 28601, \"sum\": 28601.0, \"min\": 28601}, \"Total Records Seen\": {\"count\": 1, \"max\": 28589362, \"sum\": 28589362.0, \"min\": 28589362}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 23, \"sum\": 23.0, \"min\": 23}}, \"EndTime\": 1597138888.531564, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 21}, \"StartTime\": 1597138866.371849}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:28 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58640.8351779 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=0 train rmse <loss>=0.791467251474\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=0 train mse <loss>=0.626420410156\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=0 train absolute_loss <loss>=0.564839233398\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:41:37 INFO 139921924822848] Iter[22] Batch [500]#011Speed: 59157.26 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=500 train rmse <loss>=0.683289819759\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=500 train mse <loss>=0.466884977786\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=500 train absolute_loss <loss>=0.471569055736\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:45 INFO 139921924822848] Iter[22] Batch [1000]#011Speed: 59016.89 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=1000 train rmse <loss>=0.662761830025\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=1000 train mse <loss>=0.439253243339\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, batch=1000 train absolute_loss <loss>=0.451354374507\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:41:50.453] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 46, \"duration\": 21919, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, train rmse <loss>=0.656041125542\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, train mse <loss>=0.430389958402\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=22, train absolute_loss <loss>=0.445187645944\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21921.951055526733, \"sum\": 21921.951055526733, \"min\": 21921.951055526733}}, \"EndTime\": 1597138910.453813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138888.53139}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:50 INFO 139921924822848] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 29901, \"sum\": 29901.0, \"min\": 29901}, \"Total Records Seen\": {\"count\": 1, \"max\": 29888833, \"sum\": 29888833.0, \"min\": 29888833}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1597138910.454063, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 22}, \"StartTime\": 1597138888.531835}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:50 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59276.0459929 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=0 train rmse <loss>=0.783787714341\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=0 train mse <loss>=0.614323181152\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=0 train absolute_loss <loss>=0.567404846191\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:58 INFO 139921924822848] Iter[23] Batch [500]#011Speed: 58711.59 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=500 train rmse <loss>=0.674419625478\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=500 train mse <loss>=0.45484183123\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:41:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=500 train absolute_loss <loss>=0.46426662343\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:07 INFO 139921924822848] Iter[23] Batch [1000]#011Speed: 58957.41 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=1000 train rmse <loss>=0.655032287054\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=1000 train mse <loss>=0.429067297083\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, batch=1000 train absolute_loss <loss>=0.44647566557\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:42:12.448] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 48, \"duration\": 21992, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, train rmse <loss>=0.647878827082\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, train mse <loss>=0.419746974581\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=23, train absolute_loss <loss>=0.439864861661\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21994.555950164795, \"sum\": 21994.555950164795, \"min\": 21994.555950164795}}, \"EndTime\": 1597138932.448952, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138910.453891}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:12 INFO 139921924822848] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 31201, \"sum\": 31201.0, \"min\": 31201}, \"Total Records Seen\": {\"count\": 1, \"max\": 31188304, \"sum\": 31188304.0, \"min\": 31188304}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 25, \"sum\": 25.0, \"min\": 25}}, \"EndTime\": 1597138932.449209, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 23}, \"StartTime\": 1597138910.454365}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:12 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59080.3418751 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=0 train rmse <loss>=0.778740226476\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=0 train mse <loss>=0.606436340332\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=0 train absolute_loss <loss>=0.570341430664\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:21 INFO 139921924822848] Iter[24] Batch [500]#011Speed: 58104.48 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=500 train rmse <loss>=0.665996351089\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=500 train mse <loss>=0.443551139664\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=500 train absolute_loss <loss>=0.457653719234\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:29 INFO 139921924822848] Iter[24] Batch [1000]#011Speed: 60361.72 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=1000 train rmse <loss>=0.64678142059\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=1000 train mse <loss>=0.418326206021\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, batch=1000 train absolute_loss <loss>=0.440296921828\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:42:34.365] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 21913, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, train rmse <loss>=0.6391926157\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, train mse <loss>=0.408567199965\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=24, train absolute_loss <loss>=0.432973043753\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21915.553092956543, \"sum\": 21915.553092956543, \"min\": 21915.553092956543}}, \"EndTime\": 1597138954.366275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138932.449029}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:34 INFO 139921924822848] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 32501, \"sum\": 32501.0, \"min\": 32501}, \"Total Records Seen\": {\"count\": 1, \"max\": 32487775, \"sum\": 32487775.0, \"min\": 32487775}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1597138954.366489, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 24}, \"StartTime\": 1597138932.450689}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:34 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59293.5047079 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=0 train rmse <loss>=0.768644357261\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=0 train mse <loss>=0.590814147949\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=0 train absolute_loss <loss>=0.561625976563\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:42:42 INFO 139921924822848] Iter[25] Batch [500]#011Speed: 59392.32 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=500 train rmse <loss>=0.657418362538\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=500 train mse <loss>=0.432198903402\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=500 train absolute_loss <loss>=0.450359361416\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:51 INFO 139921924822848] Iter[25] Batch [1000]#011Speed: 60246.41 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=1000 train rmse <loss>=0.6382835509\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=1000 train mse <loss>=0.40740589135\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, batch=1000 train absolute_loss <loss>=0.433118762707\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:42:56.075] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 52, \"duration\": 21707, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, train rmse <loss>=0.63049748065\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, train mse <loss>=0.397527073106\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=25, train absolute_loss <loss>=0.425472729633\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21709.71417427063, \"sum\": 21709.71417427063, \"min\": 21709.71417427063}}, \"EndTime\": 1597138976.076481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138954.366331}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:56 INFO 139921924822848] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 33801, \"sum\": 33801.0, \"min\": 33801}, \"Total Records Seen\": {\"count\": 1, \"max\": 33787246, \"sum\": 33787246.0, \"min\": 33787246}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 27, \"sum\": 27.0, \"min\": 27}}, \"EndTime\": 1597138976.076735, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 25}, \"StartTime\": 1597138954.366733}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:56 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59855.4809951 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=0 train rmse <loss>=0.756938324728\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=0 train mse <loss>=0.572955627441\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:42:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=0 train absolute_loss <loss>=0.549519836426\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:04 INFO 139921924822848] Iter[26] Batch [500]#011Speed: 58797.56 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=500 train rmse <loss>=0.649034936216\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=500 train mse <loss>=0.421246348429\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=500 train absolute_loss <loss>=0.442969574279\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:12 INFO 139921924822848] Iter[26] Batch [1000]#011Speed: 59686.98 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=1000 train rmse <loss>=0.630042335585\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=1000 train mse <loss>=0.396953344629\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:12 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, batch=1000 train absolute_loss <loss>=0.425912819328\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:43:17.995] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 54, \"duration\": 21916, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, train rmse <loss>=0.622155475676\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, train mse <loss>=0.387077435913\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=26, train absolute_loss <loss>=0.418106236596\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21918.224096298218, \"sum\": 21918.224096298218, \"min\": 21918.224096298218}}, \"EndTime\": 1597138997.99643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138976.076557}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:17 INFO 139921924822848] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 35101, \"sum\": 35101.0, \"min\": 35101}, \"Total Records Seen\": {\"count\": 1, \"max\": 35086717, \"sum\": 35086717.0, \"min\": 35086717}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1597138997.99664, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 26}, \"StartTime\": 1597138976.078177}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:17 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59286.3088518 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:18 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=0 train rmse <loss>=0.744977650147\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:18 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=0 train mse <loss>=0.554991699219\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:18 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=0 train absolute_loss <loss>=0.536571350098\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:26 INFO 139921924822848] Iter[27] Batch [500]#011Speed: 58861.70 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=500 train rmse <loss>=0.641078457572\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=500 train mse <loss>=0.410981588763\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=500 train absolute_loss <loss>=0.43595162903\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:34 INFO 139921924822848] Iter[27] Batch [1000]#011Speed: 59290.12 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=1000 train rmse <loss>=0.622313528797\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=1000 train mse <loss>=0.387274128124\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:34 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, batch=1000 train absolute_loss <loss>=0.419226587408\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:43:39.930] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 21930, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, train rmse <loss>=0.614369202669\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, train mse <loss>=0.377449517188\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=27, train absolute_loss <loss>=0.411330146226\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21932.7290058136, \"sum\": 21932.7290058136, \"min\": 21932.7290058136}}, \"EndTime\": 1597139019.930907, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597138997.996501}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:39 INFO 139921924822848] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36401, \"sum\": 36401.0, \"min\": 36401}, \"Total Records Seen\": {\"count\": 1, \"max\": 36386188, \"sum\": 36386188.0, \"min\": 36386188}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 29, \"sum\": 29.0, \"min\": 29}}, \"EndTime\": 1597139019.931108, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 27}, \"StartTime\": 1597138997.998149}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:39 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59247.1206006 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=0 train rmse <loss>=0.733125807558\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=0 train mse <loss>=0.537473449707\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=0 train absolute_loss <loss>=0.52288079834\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:43:48 INFO 139921924822848] Iter[28] Batch [500]#011Speed: 58513.84 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=500 train rmse <loss>=0.633761737455\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=500 train mse <loss>=0.401653939862\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=500 train absolute_loss <loss>=0.42970331109\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:56 INFO 139921924822848] Iter[28] Batch [1000]#011Speed: 59478.85 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=1000 train rmse <loss>=0.615324748406\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=1000 train mse <loss>=0.378624546001\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:43:56 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, batch=1000 train absolute_loss <loss>=0.413558651493\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:44:01.959] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 58, \"duration\": 22027, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, train rmse <loss>=0.607318227615\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, train mse <loss>=0.368835429594\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=28, train absolute_loss <loss>=0.405555933462\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22029.36100959778, \"sum\": 22029.36100959778, \"min\": 22029.36100959778}}, \"EndTime\": 1597139041.960718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139019.930976}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:01 INFO 139921924822848] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 37701, \"sum\": 37701.0, \"min\": 37701}, \"Total Records Seen\": {\"count\": 1, \"max\": 37685659, \"sum\": 37685659.0, \"min\": 37685659}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1597139041.960955, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 28}, \"StartTime\": 1597139019.931328}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:01 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58987.1158545 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=0 train rmse <loss>=0.722509571263\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=0 train mse <loss>=0.522020080566\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=0 train absolute_loss <loss>=0.509471801758\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:10 INFO 139921924822848] Iter[29] Batch [500]#011Speed: 57912.18 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=500 train rmse <loss>=0.627335380504\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=500 train mse <loss>=0.393549679632\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=500 train absolute_loss <loss>=0.424652336684\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:19 INFO 139921924822848] Iter[29] Batch [1000]#011Speed: 59516.55 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=1000 train rmse <loss>=0.609211141906\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=1000 train mse <loss>=0.371138215422\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, batch=1000 train absolute_loss <loss>=0.409226065591\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:44:24.029] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 60, \"duration\": 22066, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, train rmse <loss>=0.60107831271\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, train mse <loss>=0.36129513801\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=29, train absolute_loss <loss>=0.400980125991\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22068.864107131958, \"sum\": 22068.864107131958, \"min\": 22068.864107131958}}, \"EndTime\": 1597139064.030107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139041.960783}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:24 INFO 139921924822848] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 39001, \"sum\": 39001.0, \"min\": 39001}, \"Total Records Seen\": {\"count\": 1, \"max\": 38985130, \"sum\": 38985130.0, \"min\": 38985130}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 31, \"sum\": 31.0, \"min\": 31}}, \"EndTime\": 1597139064.030335, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 29}, \"StartTime\": 1597139041.961213}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:24 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58881.5579134 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=0 train rmse <loss>=0.714919651025\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=0 train mse <loss>=0.511110107422\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=0 train absolute_loss <loss>=0.499535430908\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:32 INFO 139921924822848] Iter[30] Batch [500]#011Speed: 58774.29 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=500 train rmse <loss>=0.621909661571\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=500 train mse <loss>=0.386771627156\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=500 train absolute_loss <loss>=0.420990872381\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:40 INFO 139921924822848] Iter[30] Batch [1000]#011Speed: 59626.86 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:40 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=1000 train rmse <loss>=0.603706920474\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:40 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=1000 train mse <loss>=0.364462045828\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:40 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, batch=1000 train absolute_loss <loss>=0.405763160606\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:44:45.906] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 21874, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, train rmse <loss>=0.595373921775\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, train mse <loss>=0.354470106729\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=30, train absolute_loss <loss>=0.397144356079\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21876.919984817505, \"sum\": 21876.919984817505, \"min\": 21876.919984817505}}, \"EndTime\": 1597139085.907496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139064.030176}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:45 INFO 139921924822848] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 40301, \"sum\": 40301.0, \"min\": 40301}, \"Total Records Seen\": {\"count\": 1, \"max\": 40284601, \"sum\": 40284601.0, \"min\": 40284601}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1597139085.907737, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 30}, \"StartTime\": 1597139064.030548}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:45 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59398.0940194 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=0 train rmse <loss>=0.707529002967\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=0 train mse <loss>=0.500597290039\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=0 train absolute_loss <loss>=0.492953277588\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:54 INFO 139921924822848] Iter[31] Batch [500]#011Speed: 59226.91 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=500 train rmse <loss>=0.616878244249\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=500 train mse <loss>=0.380538768228\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:44:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=500 train absolute_loss <loss>=0.41789369156\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:45:02 INFO 139921924822848] Iter[31] Batch [1000]#011Speed: 59368.85 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:02 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=1000 train rmse <loss>=0.598206568446\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:02 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=1000 train mse <loss>=0.357851098532\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:02 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, batch=1000 train absolute_loss <loss>=0.402092561156\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:45:07.811] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 64, \"duration\": 21900, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, train rmse <loss>=0.589707497257\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, train mse <loss>=0.347754932321\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=31, train absolute_loss <loss>=0.393173172866\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21902.472972869873, \"sum\": 21902.472972869873, \"min\": 21902.472972869873}}, \"EndTime\": 1597139107.811703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139085.907569}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:07 INFO 139921924822848] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 41601, \"sum\": 41601.0, \"min\": 41601}, \"Total Records Seen\": {\"count\": 1, \"max\": 41584072, \"sum\": 41584072.0, \"min\": 41584072}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 33, \"sum\": 33.0, \"min\": 33}}, \"EndTime\": 1597139107.811902, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 31}, \"StartTime\": 1597139085.909202}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:07 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59328.9845091 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=0 train rmse <loss>=0.697685622807\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=0 train mse <loss>=0.486765228271\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:07 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=0 train absolute_loss <loss>=0.484674865723\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:16 INFO 139921924822848] Iter[32] Batch [500]#011Speed: 58836.79 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=500 train rmse <loss>=0.611524532241\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=500 train mse <loss>=0.373962253532\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=500 train absolute_loss <loss>=0.414197198377\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:24 INFO 139921924822848] Iter[32] Batch [1000]#011Speed: 58913.50 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=1000 train rmse <loss>=0.592471938677\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=1000 train mse <loss>=0.35102299812\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:24 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, batch=1000 train absolute_loss <loss>=0.39777959263\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:45:29.810] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 66, \"duration\": 21996, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, train rmse <loss>=0.583907785027\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, train mse <loss>=0.340948301415\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=32, train absolute_loss <loss>=0.388751967985\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21998.59595298767, \"sum\": 21998.59595298767, \"min\": 21998.59595298767}}, \"EndTime\": 1597139129.810739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139107.811772}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:29 INFO 139921924822848] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 42901, \"sum\": 42901.0, \"min\": 42901}, \"Total Records Seen\": {\"count\": 1, \"max\": 42883543, \"sum\": 42883543.0, \"min\": 42883543}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1597139129.810974, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 32}, \"StartTime\": 1597139107.81211}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:29 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59069.583631 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=0 train rmse <loss>=0.687471057629\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=0 train mse <loss>=0.472616455078\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=0 train absolute_loss <loss>=0.476628753662\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:38 INFO 139921924822848] Iter[33] Batch [500]#011Speed: 59205.17 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=500 train rmse <loss>=0.605875707614\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=500 train mse <loss>=0.367085373077\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=500 train absolute_loss <loss>=0.40991436731\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:46 INFO 139921924822848] Iter[33] Batch [1000]#011Speed: 59311.78 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=1000 train rmse <loss>=0.586665652884\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=1000 train mse <loss>=0.344176588274\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:46 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, batch=1000 train absolute_loss <loss>=0.393130722275\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:45:51.782] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 21965, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, train rmse <loss>=0.578115579435\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, train mse <loss>=0.334217623185\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=33, train absolute_loss <loss>=0.384131085393\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21970.3369140625, \"sum\": 21970.3369140625, \"min\": 21970.3369140625}}, \"EndTime\": 1597139151.783151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139129.810811}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:51 INFO 139921924822848] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 44201, \"sum\": 44201.0, \"min\": 44201}, \"Total Records Seen\": {\"count\": 1, \"max\": 44183014, \"sum\": 44183014.0, \"min\": 44183014}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 35, \"sum\": 35.0, \"min\": 35}}, \"EndTime\": 1597139151.783337, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 33}, \"StartTime\": 1597139129.812786}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:51 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59145.7426268 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=0 train rmse <loss>=0.678513106285\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=0 train mse <loss>=0.4603800354\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:45:51 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=0 train absolute_loss <loss>=0.471282714844\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:00 INFO 139921924822848] Iter[34] Batch [500]#011Speed: 59094.38 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=500 train rmse <loss>=0.600204892533\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=500 train mse <loss>=0.36024591302\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:00 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=500 train absolute_loss <loss>=0.405454005198\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:46:08 INFO 139921924822848] Iter[34] Batch [1000]#011Speed: 59606.00 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=1000 train rmse <loss>=0.581003269119\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=1000 train mse <loss>=0.337564798727\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, batch=1000 train absolute_loss <loss>=0.388506663137\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:46:13.601] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 70, \"duration\": 21815, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, train rmse <loss>=0.572512357056\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, train mse <loss>=0.327770398982\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=34, train absolute_loss <loss>=0.379610840618\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21817.226886749268, \"sum\": 21817.226886749268, \"min\": 21817.226886749268}}, \"EndTime\": 1597139173.602053, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139151.783212}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:13 INFO 139921924822848] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 45501, \"sum\": 45501.0, \"min\": 45501}, \"Total Records Seen\": {\"count\": 1, \"max\": 45482485, \"sum\": 45482485.0, \"min\": 45482485}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1597139173.602278, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 34}, \"StartTime\": 1597139151.784799}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:13 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59560.7304253 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=0 train rmse <loss>=0.671266916868\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=0 train mse <loss>=0.450599273682\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:13 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=0 train absolute_loss <loss>=0.468512817383\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:22 INFO 139921924822848] Iter[35] Batch [500]#011Speed: 58492.05 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=500 train rmse <loss>=0.594746982042\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=500 train mse <loss>=0.353723972648\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:22 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=500 train absolute_loss <loss>=0.4012421688\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:30 INFO 139921924822848] Iter[35] Batch [1000]#011Speed: 59556.55 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=1000 train rmse <loss>=0.575655497464\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=1000 train mse <loss>=0.33137925176\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:30 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, batch=1000 train absolute_loss <loss>=0.384258462942\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:46:35.639] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 72, \"duration\": 22034, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, train rmse <loss>=0.567248460299\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, train mse <loss>=0.321770815711\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=35, train absolute_loss <loss>=0.375507507911\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22036.077976226807, \"sum\": 22036.077976226807, \"min\": 22036.077976226807}}, \"EndTime\": 1597139195.639874, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139173.602122}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:35 INFO 139921924822848] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 46801, \"sum\": 46801.0, \"min\": 46801}, \"Total Records Seen\": {\"count\": 1, \"max\": 46781956, \"sum\": 46781956.0, \"min\": 46781956}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 37, \"sum\": 37.0, \"min\": 37}}, \"EndTime\": 1597139195.640066, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 35}, \"StartTime\": 1597139173.603768}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:35 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58969.2851958 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=0 train rmse <loss>=0.665312793565\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=0 train mse <loss>=0.442641113281\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:35 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=0 train absolute_loss <loss>=0.467132263184\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:44 INFO 139921924822848] Iter[36] Batch [500]#011Speed: 58769.58 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=500 train rmse <loss>=0.589675721043\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=500 train mse <loss>=0.347717455988\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:44 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=500 train absolute_loss <loss>=0.397590717247\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:52 INFO 139921924822848] Iter[36] Batch [1000]#011Speed: 59597.41 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=1000 train rmse <loss>=0.570745238351\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=1000 train mse <loss>=0.325750127101\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:52 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, batch=1000 train absolute_loss <loss>=0.380595977753\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:46:57.601] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 21959, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, train rmse <loss>=0.562442910942\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, train mse <loss>=0.316342028069\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=36, train absolute_loss <loss>=0.372026348619\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21961.292028427124, \"sum\": 21961.292028427124, \"min\": 21961.292028427124}}, \"EndTime\": 1597139217.601625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139195.639936}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:57 INFO 139921924822848] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 48101, \"sum\": 48101.0, \"min\": 48101}, \"Total Records Seen\": {\"count\": 1, \"max\": 48081427, \"sum\": 48081427.0, \"min\": 48081427}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1597139217.601867, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 36}, \"StartTime\": 1597139195.640303}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:57 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59169.8923776 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=0 train rmse <loss>=0.659207944996\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=0 train mse <loss>=0.434555114746\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:46:57 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=0 train absolute_loss <loss>=0.464301330566\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:47:06 INFO 139921924822848] Iter[37] Batch [500]#011Speed: 57780.04 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=500 train rmse <loss>=0.585084226149\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=500 train mse <loss>=0.342323551688\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:06 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=500 train absolute_loss <loss>=0.394685873005\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:14 INFO 139921924822848] Iter[37] Batch [1000]#011Speed: 59865.79 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=1000 train rmse <loss>=0.56632663366\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=1000 train mse <loss>=0.320725855992\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:14 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, batch=1000 train absolute_loss <loss>=0.377656435392\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:47:19.652] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 76, \"duration\": 22048, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, train rmse <loss>=0.558160472869\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, train mse <loss>=0.311543113474\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=37, train absolute_loss <loss>=0.369321328336\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22050.62484741211, \"sum\": 22050.62484741211, \"min\": 22050.62484741211}}, \"EndTime\": 1597139239.652778, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139217.601694}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:19 INFO 139921924822848] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 49401, \"sum\": 49401.0, \"min\": 49401}, \"Total Records Seen\": {\"count\": 1, \"max\": 49380898, \"sum\": 49380898.0, \"min\": 49380898}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 39, \"sum\": 39.0, \"min\": 39}}, \"EndTime\": 1597139239.652968, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 37}, \"StartTime\": 1597139217.602124}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:19 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58930.3727042 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=0 train rmse <loss>=0.651026903478\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=0 train mse <loss>=0.423836029053\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:19 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=0 train absolute_loss <loss>=0.456590179443\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:28 INFO 139921924822848] Iter[38] Batch [500]#011Speed: 58490.82 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=500 train rmse <loss>=0.580916924066\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=500 train mse <loss>=0.337464472666\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:28 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=500 train absolute_loss <loss>=0.392449173796\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:36 INFO 139921924822848] Iter[38] Batch [1000]#011Speed: 59910.60 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=1000 train rmse <loss>=0.562332885009\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=1000 train mse <loss>=0.316218273563\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:36 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, batch=1000 train absolute_loss <loss>=0.375319769342\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:47:41.516] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 78, \"duration\": 21860, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, train rmse <loss>=0.554347507053\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, train mse <loss>=0.307301158576\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=38, train absolute_loss <loss>=0.367298062369\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21862.580060958862, \"sum\": 21862.580060958862, \"min\": 21862.580060958862}}, \"EndTime\": 1597139261.51706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139239.65284}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:41 INFO 139921924822848] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 50701, \"sum\": 50701.0, \"min\": 50701}, \"Total Records Seen\": {\"count\": 1, \"max\": 50680369, \"sum\": 50680369.0, \"min\": 50680369}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1597139261.517259, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 38}, \"StartTime\": 1597139239.654453}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:41 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59437.2437818 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=0 train rmse <loss>=0.64112501923\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=0 train mse <loss>=0.411041290283\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:41 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=0 train absolute_loss <loss>=0.444628326416\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:50 INFO 139921924822848] Iter[39] Batch [500]#011Speed: 58786.26 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=500 train rmse <loss>=0.57694558378\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=500 train mse <loss>=0.332866206643\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:50 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=500 train absolute_loss <loss>=0.390371817461\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:58 INFO 139921924822848] Iter[39] Batch [1000]#011Speed: 59578.02 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=1000 train rmse <loss>=0.558568878243\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=1000 train mse <loss>=0.311999191741\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:47:58 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, batch=1000 train absolute_loss <loss>=0.37317973759\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:48:03.423] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 21902, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, train rmse <loss>=0.550809786527\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, train mse <loss>=0.303391420934\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=39, train absolute_loss <loss>=0.365559876897\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21904.804944992065, \"sum\": 21904.804944992065, \"min\": 21904.804944992065}}, \"EndTime\": 1597139283.423606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139261.517123}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:03 INFO 139921924822848] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 52001, \"sum\": 52001.0, \"min\": 52001}, \"Total Records Seen\": {\"count\": 1, \"max\": 51979840, \"sum\": 51979840.0, \"min\": 51979840}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 41, \"sum\": 41.0, \"min\": 41}}, \"EndTime\": 1597139283.423839, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 39}, \"StartTime\": 1597139261.518772}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:03 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59322.546444 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=0 train rmse <loss>=0.63259658257\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=0 train mse <loss>=0.400178436279\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:03 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=0 train absolute_loss <loss>=0.435308441162\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:48:11 INFO 139921924822848] Iter[40] Batch [500]#011Speed: 58748.52 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:11 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=500 train rmse <loss>=0.57293351851\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:11 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=500 train mse <loss>=0.328252816632\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:11 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=500 train absolute_loss <loss>=0.387960089586\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:20 INFO 139921924822848] Iter[40] Batch [1000]#011Speed: 60046.60 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=1000 train rmse <loss>=0.554840633394\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=1000 train mse <loss>=0.307848128465\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:20 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, batch=1000 train absolute_loss <loss>=0.370866575533\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:48:25.246] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 82, \"duration\": 21820, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, train rmse <loss>=0.547345682653\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, train mse <loss>=0.299587296319\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=40, train absolute_loss <loss>=0.363731264343\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21823.084115982056, \"sum\": 21823.084115982056, \"min\": 21823.084115982056}}, \"EndTime\": 1597139305.247209, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139283.423681}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:25 INFO 139921924822848] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 53301, \"sum\": 53301.0, \"min\": 53301}, \"Total Records Seen\": {\"count\": 1, \"max\": 53279311, \"sum\": 53279311.0, \"min\": 53279311}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1597139305.247441, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 40}, \"StartTime\": 1597139283.42409}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:25 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59544.6173186 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=0 train rmse <loss>=0.626262713701\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=0 train mse <loss>=0.392204986572\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:25 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=0 train absolute_loss <loss>=0.429913360596\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:33 INFO 139921924822848] Iter[41] Batch [500]#011Speed: 58617.64 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:33 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=500 train rmse <loss>=0.568802989879\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:33 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=500 train mse <loss>=0.323536841295\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:33 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=500 train absolute_loss <loss>=0.384994342735\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:42 INFO 139921924822848] Iter[41] Batch [1000]#011Speed: 59320.28 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=1000 train rmse <loss>=0.551081568591\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=1000 train mse <loss>=0.303690895241\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:42 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, batch=1000 train absolute_loss <loss>=0.368246339842\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:48:47.299] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 84, \"duration\": 22050, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, train rmse <loss>=0.543885174418\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, train mse <loss>=0.295811082951\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=41, train absolute_loss <loss>=0.361648352497\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22052.342891693115, \"sum\": 22052.342891693115, \"min\": 22052.342891693115}}, \"EndTime\": 1597139327.300075, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139305.247277}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:47 INFO 139921924822848] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 54601, \"sum\": 54601.0, \"min\": 54601}, \"Total Records Seen\": {\"count\": 1, \"max\": 54578782, \"sum\": 54578782.0, \"min\": 54578782}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 43, \"sum\": 43.0, \"min\": 43}}, \"EndTime\": 1597139327.300266, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 41}, \"StartTime\": 1597139305.247701}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:47 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58925.7689099 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=0 train rmse <loss>=0.620402770116\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=0 train mse <loss>=0.384899597168\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:47 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=0 train absolute_loss <loss>=0.425366546631\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:55 INFO 139921924822848] Iter[42] Batch [500]#011Speed: 58763.99 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:55 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=500 train rmse <loss>=0.564607107624\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:55 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=500 train mse <loss>=0.31878118598\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:48:55 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=500 train absolute_loss <loss>=0.381565346175\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:04 INFO 139921924822848] Iter[42] Batch [1000]#011Speed: 59520.81 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=1000 train rmse <loss>=0.547334290577\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=1000 train mse <loss>=0.299574825641\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:04 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, batch=1000 train absolute_loss <loss>=0.365431563493\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:49:09.300] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 21997, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, train rmse <loss>=0.540473014938\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, train mse <loss>=0.292111079876\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=42, train absolute_loss <loss>=0.35941434744\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21999.984979629517, \"sum\": 21999.984979629517, \"min\": 21999.984979629517}}, \"EndTime\": 1597139349.300507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139327.300137}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:09 INFO 139921924822848] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 55901, \"sum\": 55901.0, \"min\": 55901}, \"Total Records Seen\": {\"count\": 1, \"max\": 55878253, \"sum\": 55878253.0, \"min\": 55878253}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1597139349.300719, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 42}, \"StartTime\": 1597139327.300495}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:09 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59065.9828445 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=0 train rmse <loss>=0.614011333945\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=0 train mse <loss>=0.377009918213\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:09 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=0 train absolute_loss <loss>=0.419968017578\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:49:17 INFO 139921924822848] Iter[43] Batch [500]#011Speed: 59130.11 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=500 train rmse <loss>=0.56044646761\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=500 train mse <loss>=0.314100243056\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:17 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=500 train absolute_loss <loss>=0.377946149815\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:26 INFO 139921924822848] Iter[43] Batch [1000]#011Speed: 60098.18 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=1000 train rmse <loss>=0.543684962141\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=1000 train mse <loss>=0.295593338058\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:26 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, batch=1000 train absolute_loss <loss>=0.362632670308\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:49:31.117] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 88, \"duration\": 21813, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, train rmse <loss>=0.537198774848\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, train mse <loss>=0.288582523698\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=43, train absolute_loss <loss>=0.357226735159\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21815.335988998413, \"sum\": 21815.335988998413, \"min\": 21815.335988998413}}, \"EndTime\": 1597139371.117593, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139349.300566}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:31 INFO 139921924822848] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 57201, \"sum\": 57201.0, \"min\": 57201}, \"Total Records Seen\": {\"count\": 1, \"max\": 57177724, \"sum\": 57177724.0, \"min\": 57177724}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}}, \"EndTime\": 1597139371.117829, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 43}, \"StartTime\": 1597139349.302207}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:31 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59565.6474951 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=0 train rmse <loss>=0.607320253758\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=0 train mse <loss>=0.368837890625\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:31 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=0 train absolute_loss <loss>=0.414067901611\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:39 INFO 139921924822848] Iter[44] Batch [500]#011Speed: 57600.85 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=500 train rmse <loss>=0.556416800312\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=500 train mse <loss>=0.309599655669\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:39 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=500 train absolute_loss <loss>=0.37437977329\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:48 INFO 139921924822848] Iter[44] Batch [1000]#011Speed: 60107.67 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=1000 train rmse <loss>=0.540222523006\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=1000 train mse <loss>=0.291840374363\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:48 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, batch=1000 train absolute_loss <loss>=0.359994760946\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:49:53.144] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 90, \"duration\": 22024, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, train rmse <loss>=0.534152392734\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, train mse <loss>=0.285318778663\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=44, train absolute_loss <loss>=0.355204709966\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22027.220964431763, \"sum\": 22027.220964431763, \"min\": 22027.220964431763}}, \"EndTime\": 1597139393.145398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139371.117667}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:53 INFO 139921924822848] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 58501, \"sum\": 58501.0, \"min\": 58501}, \"Total Records Seen\": {\"count\": 1, \"max\": 58477195, \"sum\": 58477195.0, \"min\": 58477195}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1597139393.145589, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 44}, \"StartTime\": 1597139371.118149}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:53 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58992.9749651 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=0 train rmse <loss>=0.600924250496\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=0 train mse <loss>=0.361109954834\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:49:53 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=0 train absolute_loss <loss>=0.408579223633\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:01 INFO 139921924822848] Iter[45] Batch [500]#011Speed: 58360.46 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=500 train rmse <loss>=0.552581496752\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=500 train mse <loss>=0.305346310553\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:01 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=500 train absolute_loss <loss>=0.370961424807\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:10 INFO 139921924822848] Iter[45] Batch [1000]#011Speed: 58791.79 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=1000 train rmse <loss>=0.537022676541\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=1000 train mse <loss>=0.288393355119\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:10 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, batch=1000 train absolute_loss <loss>=0.357605988037\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:50:15.315] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 22167, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, train rmse <loss>=0.531403875238\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, train mse <loss>=0.282390078618\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=45, train absolute_loss <loss>=0.353449100224\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22168.948888778687, \"sum\": 22168.948888778687, \"min\": 22168.948888778687}}, \"EndTime\": 1597139415.316042, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139393.145461}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:15 INFO 139921924822848] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 59801, \"sum\": 59801.0, \"min\": 59801}, \"Total Records Seen\": {\"count\": 1, \"max\": 59776666, \"sum\": 59776666.0, \"min\": 59776666}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 47, \"sum\": 47.0, \"min\": 47}}, \"EndTime\": 1597139415.316278, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 45}, \"StartTime\": 1597139393.147067}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:15 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58615.7277741 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=0 train rmse <loss>=0.595122719299\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=0 train mse <loss>=0.354171051025\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:15 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=0 train absolute_loss <loss>=0.403766082764\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:50:23 INFO 139921924822848] Iter[46] Batch [500]#011Speed: 58855.98 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=500 train rmse <loss>=0.548964356558\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=500 train mse <loss>=0.301361864771\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:23 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=500 train absolute_loss <loss>=0.367738884048\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:32 INFO 139921924822848] Iter[46] Batch [1000]#011Speed: 59684.82 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=1000 train rmse <loss>=0.53414769556\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=1000 train mse <loss>=0.285313760672\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:32 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, batch=1000 train absolute_loss <loss>=0.355554585155\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:50:37.253] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 94, \"duration\": 21934, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, train rmse <loss>=0.528996473184\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, train mse <loss>=0.279837268642\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=46, train absolute_loss <loss>=0.352066312209\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21936.521768569946, \"sum\": 21936.521768569946, \"min\": 21936.521768569946}}, \"EndTime\": 1597139437.254542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139415.31611}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:37 INFO 139921924822848] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 61101, \"sum\": 61101.0, \"min\": 61101}, \"Total Records Seen\": {\"count\": 1, \"max\": 61076137, \"sum\": 61076137.0, \"min\": 61076137}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1597139437.254782, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 46}, \"StartTime\": 1597139415.317985}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:37 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=59236.6813802 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=0 train rmse <loss>=0.589847190594\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=0 train mse <loss>=0.347919708252\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:37 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=0 train absolute_loss <loss>=0.399717468262\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:45 INFO 139921924822848] Iter[47] Batch [500]#011Speed: 58171.39 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=500 train rmse <loss>=0.545553783207\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=500 train mse <loss>=0.297628930372\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:45 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=500 train absolute_loss <loss>=0.364749003945\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:54 INFO 139921924822848] Iter[47] Batch [1000]#011Speed: 59318.50 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=1000 train rmse <loss>=0.53164685067\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=1000 train mse <loss>=0.282648373828\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:54 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, batch=1000 train absolute_loss <loss>=0.353967686946\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:50:59.370] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 96, \"duration\": 22112, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, train rmse <loss>=0.526941557343\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, train mse <loss>=0.277667404856\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=47, train absolute_loss <loss>=0.351122444247\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22114.336013793945, \"sum\": 22114.336013793945, \"min\": 22114.336013793945}}, \"EndTime\": 1597139459.370684, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139437.254617}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:59 INFO 139921924822848] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 62401, \"sum\": 62401.0, \"min\": 62401}, \"Total Records Seen\": {\"count\": 1, \"max\": 62375608, \"sum\": 62375608.0, \"min\": 62375608}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 49, \"sum\": 49.0, \"min\": 49}}, \"EndTime\": 1597139459.370898, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 47}, \"StartTime\": 1597139437.256318}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:59 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58760.5607206 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=0 train rmse <loss>=0.584949322975\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=0 train mse <loss>=0.342165710449\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:50:59 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=0 train absolute_loss <loss>=0.396244476318\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:08 INFO 139921924822848] Iter[48] Batch [500]#011Speed: 58062.83 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=500 train rmse <loss>=0.542308371659\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=500 train mse <loss>=0.294098369971\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:08 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=500 train absolute_loss <loss>=0.361961552664\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:16 INFO 139921924822848] Iter[48] Batch [1000]#011Speed: 59043.67 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=1000 train rmse <loss>=0.529539207989\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=1000 train mse <loss>=0.280411772798\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:16 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, batch=1000 train absolute_loss <loss>=0.352853384287\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:51:21.451] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 22077, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, train rmse <loss>=0.525204468815\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, train mse <loss>=0.275839734063\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=48, train absolute_loss <loss>=0.350565919671\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22079.190969467163, \"sum\": 22079.190969467163, \"min\": 22079.190969467163}}, \"EndTime\": 1597139481.451667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139459.370754}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:21 INFO 139921924822848] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 63701, \"sum\": 63701.0, \"min\": 63701}, \"Total Records Seen\": {\"count\": 1, \"max\": 63675079, \"sum\": 63675079.0, \"min\": 63675079}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1597139481.45192, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 48}, \"StartTime\": 1597139459.372446}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:21 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58853.9249195 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=0 train rmse <loss>=0.580351498084\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=0 train mse <loss>=0.336807861328\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:21 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=0 train absolute_loss <loss>=0.39278237915\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[08/11/2020 09:51:29 INFO 139921924822848] Iter[49] Batch [500]#011Speed: 58783.04 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=500 train rmse <loss>=0.539163863528\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=500 train mse <loss>=0.290697671734\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:29 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=500 train absolute_loss <loss>=0.359302599026\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:38 INFO 139921924822848] Iter[49] Batch [1000]#011Speed: 59180.36 samples/sec\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=1000 train rmse <loss>=0.527778380035\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=1000 train mse <loss>=0.278550018432\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:38 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, batch=1000 train absolute_loss <loss>=0.352147466181\u001b[0m\n",
      "\u001b[34m[2020-08-11 09:51:43.502] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 100, \"duration\": 22047, \"num_examples\": 1300, \"num_bytes\": 93551760}\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, train rmse <loss>=0.523685942359\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, train mse <loss>=0.274246966224\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] #quality_metric: host=algo-1, epoch=49, train absolute_loss <loss>=0.350294426528\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] #quality_metric: host=algo-1, train rmse <loss>=0.523685942359\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] #quality_metric: host=algo-1, train mse <loss>=0.274246966224\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] #quality_metric: host=algo-1, train absolute_loss <loss>=0.350294426528\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22049.90005493164, \"sum\": 22049.90005493164, \"min\": 22049.90005493164}}, \"EndTime\": 1597139503.503392, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139481.451746}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1300, \"sum\": 1300.0, \"min\": 1300}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Total Batches Seen\": {\"count\": 1, \"max\": 65001, \"sum\": 65001.0, \"min\": 65001}, \"Total Records Seen\": {\"count\": 1, \"max\": 64974550, \"sum\": 64974550.0, \"min\": 64974550}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 1299471, \"sum\": 1299471.0, \"min\": 1299471}, \"Reset Count\": {\"count\": 1, \"max\": 51, \"sum\": 51.0, \"min\": 51}}, \"EndTime\": 1597139503.503584, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\", \"epoch\": 49}, \"StartTime\": 1597139481.453464}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] #throughput_metric: host=algo-1, train throughput=58932.321219 records/second\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 WARNING 139921924822848] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:43 INFO 139921924822848] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 132.56001472473145, \"sum\": 132.56001472473145, \"min\": 132.56001472473145}}, \"EndTime\": 1597139503.636443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139503.503458}\n",
      "\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:44 INFO 139921924822848] Saved checkpoint to \"/tmp/tmpa_T2Iv/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[08/11/2020 09:51:45 INFO 139921924822848] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1104061.8600845337, \"sum\": 1104061.8600845337, \"min\": 1104061.8600845337}, \"setuptime\": {\"count\": 1, \"max\": 58.21108818054199, \"sum\": 58.21108818054199, \"min\": 58.21108818054199}}, \"EndTime\": 1597139505.117569, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"factorization-machines\"}, \"StartTime\": 1597139503.636564}\n",
      "\u001b[0m\n",
      "\n",
      "2020-08-11 09:51:48 Uploading - Uploading generated training model\n",
      "2020-08-11 09:52:26 Completed - Training job completed\n",
      "Training seconds: 1205\n",
      "Billable seconds: 1205\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import boto3 \n",
    "\n",
    "region = boto3.Session().region_name\n",
    "crole = 'AmazonSageMaker-ExecutionRole-20200603T105247'\n",
    "\n",
    "print(sagemaker.estimator.Estimator)\n",
    "fm = sagemaker.estimator.Estimator(image_uri=container,\n",
    "                                   role=crole, \n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "                      feature_dim=nFeatures,\n",
    "                      predictor_type='regressor',\n",
    "                      mini_batch_size=1000,\n",
    "                      num_factors=64,\n",
    "                      epochs=50)\n",
    "\n",
    "fm.fit({'train': train_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "fm_predictor = fm.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import BaseSerializer\n",
    "import json \n",
    "\n",
    "class FMSerializer(BaseSerializer): \n",
    "    CONTENT_TYPE='application/json'\n",
    "    def serialize(self, data):\n",
    "        js = {'instances': []}\n",
    "        for row in data:\n",
    "            js['instances'].append({'features': row.tolist()})\n",
    "        #print js\n",
    "        return json.dumps(js)\n",
    "\n",
    "fm_predictor.serializer = FMSerializer()\n",
    "fm_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "import numpy \n",
    "\n",
    "X_test_arr = X_test[3].toarray(order='C')\n",
    "print(X_test_arr)\n",
    "\n",
    "result = fm_predictor.predict(X_test_arr) \n",
    "y_pred = [] \n",
    "for p in result['predictions']: \n",
    "    y_pred.append(p['score'])\n",
    "\n",
    "rmse = numpy.sqrt(numpy.mean((y_pred-Y_test[3])**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3286135196685791"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>uid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>video_release_date</th>\n",
       "      <th>imdb url</th>\n",
       "      <th>unknown</th>\n",
       "      <th>Action</th>\n",
       "      <th>...</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>age_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46289</th>\n",
       "      <td>244</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>880602451</td>\n",
       "      <td>Smilla's Sense of Snow (1997)</td>\n",
       "      <td>14-Mar-1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Smilla%27s%20...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46290</th>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>879372325</td>\n",
       "      <td>Stargate (1994)</td>\n",
       "      <td>01-Jan-1994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Stargate%20(1...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46291</th>\n",
       "      <td>286</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>876522316</td>\n",
       "      <td>English Patient, The (1996)</td>\n",
       "      <td>15-Nov-1996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?English%20Pat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46292</th>\n",
       "      <td>303</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>879485184</td>\n",
       "      <td>Ulee's Gold (1997)</td>\n",
       "      <td>01-Jan-1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Ulee%27s+Gold...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46293</th>\n",
       "      <td>291</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>874833936</td>\n",
       "      <td>Absolute Power (1997)</td>\n",
       "      <td>14-Feb-1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Absolute%20Po...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46374</th>\n",
       "      <td>916</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>880843838</td>\n",
       "      <td>Lost in Space (1998)</td>\n",
       "      <td>27-Mar-1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/Title?Lost+in+Space+(1998)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46375</th>\n",
       "      <td>910</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>881421019</td>\n",
       "      <td>Nil By Mouth (1997)</td>\n",
       "      <td>06-Feb-1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/Title?Nil+By+Mouth+(1997)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46376</th>\n",
       "      <td>923</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>880387707</td>\n",
       "      <td>Raise the Red Lantern (1991)</td>\n",
       "      <td>01-Jan-1991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Da%20Hong%20D...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46377</th>\n",
       "      <td>917</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>882911567</td>\n",
       "      <td>Mercury Rising (1998)</td>\n",
       "      <td>27-Mar-1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/Title?Mercury+Rising+(1998)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46378</th>\n",
       "      <td>936</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>886833148</td>\n",
       "      <td>Brassed Off (1996)</td>\n",
       "      <td>13-Jun-1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Brassed%20Off...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       iid  uid  rating  timestamp                          title  \\\n",
       "46289  244    3       5  880602451  Smilla's Sense of Snow (1997)   \n",
       "46290   62    3       3  879372325                Stargate (1994)   \n",
       "46291  286    3       2  876522316    English Patient, The (1996)   \n",
       "46292  303    3       3  879485184             Ulee's Gold (1997)   \n",
       "46293  291    3       3  874833936          Absolute Power (1997)   \n",
       "...    ...  ...     ...        ...                            ...   \n",
       "46374  916    3       3  880843838           Lost in Space (1998)   \n",
       "46375  910    3       2  881421019            Nil By Mouth (1997)   \n",
       "46376  923    3       4  880387707   Raise the Red Lantern (1991)   \n",
       "46377  917    3       1  882911567          Mercury Rising (1998)   \n",
       "46378  936    3       4  886833148             Brassed Off (1996)   \n",
       "\n",
       "      release_date  video_release_date  \\\n",
       "46289  14-Mar-1997                 NaN   \n",
       "46290  01-Jan-1994                 NaN   \n",
       "46291  15-Nov-1996                 NaN   \n",
       "46292  01-Jan-1997                 NaN   \n",
       "46293  14-Feb-1997                 NaN   \n",
       "...            ...                 ...   \n",
       "46374  27-Mar-1998                 NaN   \n",
       "46375  06-Feb-1998                 NaN   \n",
       "46376  01-Jan-1991                 NaN   \n",
       "46377  27-Mar-1998                 NaN   \n",
       "46378  13-Jun-1997                 NaN   \n",
       "\n",
       "                                                imdb url  unknown  Action  \\\n",
       "46289  http://us.imdb.com/M/title-exact?Smilla%27s%20...        0       1   \n",
       "46290  http://us.imdb.com/M/title-exact?Stargate%20(1...        0       1   \n",
       "46291  http://us.imdb.com/M/title-exact?English%20Pat...        0       0   \n",
       "46292  http://us.imdb.com/M/title-exact?Ulee%27s+Gold...        0       0   \n",
       "46293  http://us.imdb.com/M/title-exact?Absolute%20Po...        0       0   \n",
       "...                                                  ...      ...     ...   \n",
       "46374      http://us.imdb.com/Title?Lost+in+Space+(1998)        0       1   \n",
       "46375       http://us.imdb.com/Title?Nil+By+Mouth+(1997)        0       0   \n",
       "46376  http://us.imdb.com/M/title-exact?Da%20Hong%20D...        0       0   \n",
       "46377     http://us.imdb.com/Title?Mercury+Rising+(1998)        0       1   \n",
       "46378  http://us.imdb.com/M/title-exact?Brassed%20Off...        0       0   \n",
       "\n",
       "       ...  Romance  Sci-Fi  Thriller  War  Western  age  gender  occupation  \\\n",
       "46289  ...        0       0         1    0        0   23       M      writer   \n",
       "46290  ...        0       1         0    0        0   23       M      writer   \n",
       "46291  ...        1       0         0    1        0   23       M      writer   \n",
       "46292  ...        0       0         0    0        0   23       M      writer   \n",
       "46293  ...        0       0         1    0        0   23       M      writer   \n",
       "...    ...      ...     ...       ...  ...      ...  ...     ...         ...   \n",
       "46374  ...        0       1         1    0        0   23       M      writer   \n",
       "46375  ...        0       0         0    0        0   23       M      writer   \n",
       "46376  ...        0       0         0    0        0   23       M      writer   \n",
       "46377  ...        0       0         1    0        0   23       M      writer   \n",
       "46378  ...        1       0         0    0        0   23       M      writer   \n",
       "\n",
       "       zipcode  age_segment  \n",
       "46289    32067            2  \n",
       "46290    32067            2  \n",
       "46291    32067            2  \n",
       "46292    32067            2  \n",
       "46293    32067            2  \n",
       "...        ...          ...  \n",
       "46374    32067            2  \n",
       "46375    32067            2  \n",
       "46376    32067            2  \n",
       "46377    32067            2  \n",
       "46378    32067            2  \n",
       "\n",
       "[90 rows x 32 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
