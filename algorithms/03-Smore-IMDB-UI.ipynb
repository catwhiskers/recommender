{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use SOTA, which updates frequently, in a very short time? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bring your own container - BYOC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.10.0)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from seaborn) (3.1.3)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from seaborn) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from seaborn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.2->seaborn) (46.1.3.post20200330)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  4.608kB\r",
      "\r\n",
      "Step 1/7 : FROM ubuntu:18.04\n",
      " ---> 2eb2d388e1a2\n",
      "Step 2/7 : RUN apt-get update &&     apt-get install -y --no-install-recommends         ca-certificates         cmake         build-essential         gcc         g++         git &&     rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 928aea19ecbf\n",
      "Step 3/7 : RUN git clone https://github.com/cnclabs/smore.git && cd smore && make\n",
      " ---> Using cache\n",
      " ---> 84c80480d45a\n",
      "Step 4/7 : RUN cp -r /smore /app/\n",
      " ---> Using cache\n",
      " ---> 3c8ea08e06a2\n",
      "Step 5/7 : ENV PATH=\"/app:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 0d175c678e3c\n",
      "Step 6/7 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 44b335f17ab3\n",
      "Step 7/7 : COPY train /app/\n",
      " ---> Using cache\n",
      " ---> f1a1f76c3add\n",
      "Successfully built f1a1f76c3add\n",
      "Successfully tagged smore-byoc:latest\n",
      "The push refers to repository [230755935769.dkr.ecr.us-east-1.amazonaws.com/smore-byoc]\n",
      "cf796395a59d: Preparing\n",
      "714816eb6d34: Preparing\n",
      "e85547b9ab30: Preparing\n",
      "e1ce31771c29: Preparing\n",
      "8682f9a74649: Preparing\n",
      "d3a6da143c91: Preparing\n",
      "83f4287e1f04: Preparing\n",
      "7ef368776582: Preparing\n",
      "d3a6da143c91: Waiting\n",
      "7ef368776582: Waiting\n",
      "83f4287e1f04: Waiting\n",
      "e1ce31771c29: Layer already exists\n",
      "714816eb6d34: Layer already exists\n",
      "cf796395a59d: Layer already exists\n",
      "8682f9a74649: Layer already exists\n",
      "e85547b9ab30: Layer already exists\n",
      "d3a6da143c91: Layer already exists\n",
      "83f4287e1f04: Layer already exists\n",
      "7ef368776582: Layer already exists\n",
      "latest: digest: sha256:ca56c9245fa2d7170821b10b8bfa72024084c606427a0979bd68bb0fe1e90558 size: 1994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd byoc/smore/\n",
    "./build_and_push.sh smore-byoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: docker_name=230755935769.dkr.ecr.us-east-1.amazonaws.com/smore-byoc\n"
     ]
    }
   ],
   "source": [
    "%env docker_name=230755935769.dkr.ecr.us-east-1.amazonaws.com/smore-byoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/bin/bash\u001b[39;49;00m\r\n",
      "\u001b[31mimage\u001b[39;49;00m=\u001b[31m$1\u001b[39;49;00m\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[33m\"\u001b[39;49;00m\u001b[31m$image\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m == \u001b[33m\"\"\u001b[39;49;00m ]\r\n",
      "\u001b[34mthen\u001b[39;49;00m\r\n",
      "    \u001b[36mecho\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mUsage: \u001b[39;49;00m\u001b[31m$0\u001b[39;49;00m\u001b[33m <image-name>\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "    \u001b[36mexit\u001b[39;49;00m \u001b[34m1\u001b[39;49;00m\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n",
      "\u001b[31maccount\u001b[39;49;00m=\u001b[34m$(\u001b[39;49;00maws sts get-caller-identity --query Account --output text\u001b[34m)\u001b[39;49;00m\r\n",
      "\u001b[31mregion\u001b[39;49;00m=\u001b[34m$(\u001b[39;49;00maws configure get region\u001b[34m)\u001b[39;49;00m\r\n",
      "\u001b[31mfullname\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31maccount\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.dkr.ecr.\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m.amazonaws.com/\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m:latest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "\r\n",
      "aws ecr describe-repositories --repository-names \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null \u001b[34m2\u001b[39;49;00m>&\u001b[34m1\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m [ \u001b[31m$?\u001b[39;49;00m -ne \u001b[34m0\u001b[39;49;00m ]\r\n",
      "\u001b[34mthen\u001b[39;49;00m\r\n",
      "    aws ecr create-repository --repository-name \u001b[33m\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m > /dev/null\r\n",
      "\u001b[34mfi\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34m$(\u001b[39;49;00maws ecr get-login --region \u001b[33m${\u001b[39;49;00m\u001b[31mregion\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m --no-include-email\u001b[34m)\u001b[39;49;00m\r\n",
      "\r\n",
      "docker build -t \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m . \r\n",
      "docker tag \u001b[33m${\u001b[39;49;00m\u001b[31mimage\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n",
      "\r\n",
      "docker push \u001b[33m${\u001b[39;49;00m\u001b[31mfullname\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./byoc/smore/build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM ubuntu:18.04\r\n",
      "\r\n",
      "\r\n",
      "RUN apt-get update && \\\r\n",
      "    apt-get install -y --no-install-recommends \\\r\n",
      "        ca-certificates \\\r\n",
      "        cmake \\\r\n",
      "        build-essential \\\r\n",
      "        gcc \\\r\n",
      "        g++ \\\r\n",
      "        git && \\\r\n",
      "    rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "RUN git clone https://github.com/cnclabs/smore.git && cd smore && make \r\n",
      "\r\n",
      "RUN cp -r /smore /app/\r\n",
      "ENV PATH=\"/app:${PATH}\"\r\n",
      "WORKDIR /app \r\n",
      "COPY train /app/\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./byoc/smore/dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from preprocessing.imdb_datareader import IMDBDataReader\n",
    "from preprocessing.smore_transformer import SmoreDataTransformer\n",
    "import pickle \n",
    "\n",
    "user_item  = pickle.load(open(\"data/user_item.p\", \"rb\")) \n",
    "\n",
    "\n",
    "users = {}\n",
    "items = {}\n",
    "train_user_item = user_item[:int(len(user_item)*0.8)]\n",
    "test_user_item = user_item[int(len(user_item)*0.8):]\n",
    "transformer = SmoreDataTransformer(users, items, train_user_item)\n",
    "X_train, Y_train, _, _, nFeatures = transformer.get_feature_vectors(users, items, train_user_item)\n",
    "X_test, Y_test,X_cold_test, Y_cold_test, nFeatures = transformer.get_feature_vectors(users, items, test_user_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: s3://recommendation-demo-yianc/sagemaker/smore-movielens/output\n"
     ]
    }
   ],
   "source": [
    "bucket = 'recommendation-demo-yianc'\n",
    "prefix = 'sagemaker/smore-movielens'\n",
    "train_key      = 'net'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "\n",
    "\n",
    "import io,boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeDatasetToSmoreTXT(X, bucket, prefix, key, Y=None):\n",
    "    f_name = key+\".txt\"\n",
    "    f = open(f_name, 'w')\n",
    "    for i in range(0, len(X)): \n",
    "        raw = X[i]\n",
    "        n1 = raw[0][0]\n",
    "        n2 = raw[1][0]\n",
    "        w = Y[i]\n",
    "        f.write(\"{} {} {}\\n\".format(n1, n2, w))\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    object_name = '{}/{}'.format(prefix, f_name)\n",
    "    response = s3_client.upload_file(f_name, bucket, object_name)\n",
    " \n",
    "    return 's3://{}/{}'.format(bucket,object_name)\n",
    "    \n",
    "train_data = writeDatasetToSmoreTXT(X_train, bucket, train_prefix, train_key,  Y_train)    \n",
    "  \n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 1554 1.0\r\n",
      "140 1588 1.0\r\n",
      "140 1601 1.0\r\n",
      "140 1476 1.0\r\n",
      "140 1463 1.0\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat net.txt | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/ec2-user/SageMaker/recommender/algorithms/model’: File exists\n",
      "mkdir: cannot create directory '/opt/ml/model': File exists\n",
      "Connections Preview:\n",
      "\t# of connection:\t80000\n",
      "Connections Loading:\n",
      "\tProgress:\t\t100.00 %\n",
      "\t# of vertex:\t\t2367\n",
      "Build the Alias Method:\n",
      "\tReconstructing Graph ...\n",
      "\tBuilding Alias Tables ...\n",
      "\tFinished.\n",
      "Model Setting:\n",
      "\tdimension:\t\t64\n",
      "Model:\n",
      "\t[DeepWalk]\n",
      "Learning Parameters:\n",
      "\twalk_times:\t\t1\n",
      "\twalk_steps:\t\t40\n",
      "\twindow_size:\t\t5\n",
      "\tnegative_samples:\t5\n",
      "\talpha:\t\t\t0.025\n",
      "\tworkers:\t\t4\n",
      "Start Training:\n",
      "\tAlpha: 0.025000\tProgress: 100.00 %\n",
      "Save Model:\n",
      "\tSave to </opt/ml/model/rep_dw.txt>\n"
     ]
    }
   ],
   "source": [
    "!mkdir ${PWD}/model\n",
    "!docker run -it -v ${PWD}:/opt/ml/input/data/train/ -v ${PWD}/model:/opt/ml/model smore-byoc train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2367 64\r\n",
      "140 -0.072039 0.0465892 -0.199141 -0.335169 -0.0614013 0.0421625 -0.0592508 -0.329073 0.320188 0.0878671 -0.102877 0.147127 0.0273161 0.0687555 -0.276137 -0.311322 0.0679498 -0.120628 0.0157252 -0.130048 -0.0641683 -0.0720416 0.0164796 -0.547624 0.159068 0.359149 0.0890028 0.0611993 -0.207284 0.139362 -0.0662043 -0.0819673 -0.129117 0.2181 -0.066151 -0.073682 -0.0373574 0.246498 -0.198885 -0.0594213 -0.0413026 -0.00376554 0.0363313 0.269003 -0.0280695 -0.102051 -0.0639101 -0.369932 0.0482646 -0.451708 -0.0996484 -0.13991 0.00758065 0.148217 -0.174237 0.121686 0.0252737 -0.211 -0.119822 0.0584288 -0.0695208 -0.0220653 -0.115582 0.0414145\r\n",
      "1554 -0.0795997 0.0433838 -0.192323 -0.311289 -0.059076 0.0497207 -0.0528664 -0.326663 0.316134 0.0956682 -0.0866561 0.143001 0.0268247 0.0682286 -0.264156 -0.306317 0.0685595 -0.12497 0.0160194 -0.120207 -0.0460327 -0.0632964 0.0179489 -0.522151 0.154211 0.341895 0.0963406 0.0641499 -0.20128 0.149819 -0.0596729 -0.0712739 -0.126534 0.214925 -0.0676229 -0.0620168 -0.0323541 0.222826 -0.18218 -0.0594407 -0.0484352 0.000787654 0.0406365 0.258307 -0.0147713 -0.0991383 -0.0596877 -0.3594 0.0493395 -0.439824 -0.103729 -0.13826 0.0126962 0.134926 -0.180695 0.122449 0.0256352 -0.199712 -0.111481 0.0683401 -0.0543801 -0.0142729 -0.116564 0.0338519\r\n",
      "1588 -0.0731576 0.0410216 -0.2088 -0.321589 -0.0371796 0.045418 -0.0659024 -0.324313 0.309442 0.103939 -0.0758633 0.14998 0.0395654 0.0681235 -0.280879 -0.297292 0.0597427 -0.13159 0.0238404 -0.116828 -0.05701 -0.0519458 0.0469393 -0.523279 0.160254 0.322175 0.0835395 0.060609 -0.24204 0.126475 -0.0645448 -0.0634143 -0.129725 0.222303 -0.0784189 -0.0827163 -0.0185603 0.237634 -0.192446 -0.0727221 -0.0198039 -0.0144121 0.0433392 0.250283 -0.0329196 -0.0804311 -0.0575379 -0.370802 0.0388089 -0.42166 -0.0917942 -0.120616 0.0133103 0.148925 -0.174567 0.136868 0.0401321 -0.205169 -0.122118 0.0632699 -0.0785231 0.00869187 -0.114311 0.038501\r\n",
      "1601 -0.0678367 0.045265 -0.202223 -0.327707 -0.052089 0.0530707 -0.0493582 -0.326778 0.335276 0.0931217 -0.0854411 0.159583 0.0381987 0.0629911 -0.282907 -0.300225 0.073343 -0.13221 0.0139401 -0.123039 -0.052261 -0.0641308 0.030493 -0.553258 0.166708 0.348488 0.0817372 0.0765869 -0.215699 0.14957 -0.0582745 -0.0778707 -0.133461 0.217766 -0.0618811 -0.0716061 -0.0315528 0.247477 -0.18903 -0.0729612 -0.0528996 0.00133449 0.0343362 0.259089 -0.0343908 -0.0924993 -0.0614159 -0.379622 0.0533759 -0.451743 -0.0996732 -0.139178 0.0130022 0.138741 -0.171061 0.110569 0.0327806 -0.197282 -0.111894 0.0646487 -0.0758134 -0.0212231 -0.116295 0.0394215\r\n",
      "cat: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "cat ${PWD}/model/rep_dw.txt | head -n 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-20 08:53:04 Starting - Starting the training job...\n",
      "2020-08-20 08:53:05 Starting - Launching requested ML instances.........\n",
      "2020-08-20 08:54:46 Starting - Preparing the instances for training...\n",
      "2020-08-20 08:55:35 Downloading - Downloading input data...\n",
      "2020-08-20 08:55:41 Training - Downloading the training image..\u001b[34mmkdir: cannot create directory '/opt/ml/model': File exists\u001b[0m\n",
      "\u001b[34mConnections Preview:\u001b[0m\n",
      "\u001b[34m#011# of connection:#0110#015#011# of connection:#01110000#015#011# of connection:#01120000#015#011# of connection:#01130000#015#011# of connection:#01140000#015#011# of connection:#01150000#015#011# of connection:#01160000#015#011# of connection:#01170000#015#011# of connection:#01179665\u001b[0m\n",
      "\u001b[34mConnections Loading:\u001b[0m\n",
      "\u001b[34m#011Progress:#011#0110.00 %#015#011Progress:#011#01112.55 %#015#011Progress:#011#01125.10 %#015#011Progress:#011#01137.66 %#015#011Progress:#011#01150.21 %#015#011Progress:#011#01162.76 %#015#011Progress:#011#01175.31 %#015#011Progress:#011#01187.87 %#015#011Progress:#011#011100.00 %#015\u001b[0m\n",
      "\u001b[34m#011# of vertex:#011#0112363\u001b[0m\n",
      "\u001b[34mBuild the Alias Method:\u001b[0m\n",
      "\u001b[34m#011Reconstructing Graph ...\u001b[0m\n",
      "\u001b[34m#011Building Alias Tables ...\u001b[0m\n",
      "\u001b[34m#011Finished.\u001b[0m\n",
      "\u001b[34mModel Setting:\u001b[0m\n",
      "\u001b[34m#011dimension:#011#01164\u001b[0m\n",
      "\u001b[34mModel:\u001b[0m\n",
      "\u001b[34m#011[DeepWalk]\u001b[0m\n",
      "\u001b[34mLearning Parameters:\u001b[0m\n",
      "\u001b[34m#011walk_times:#011#0111\u001b[0m\n",
      "\u001b[34m#011walk_steps:#011#01140\u001b[0m\n",
      "\u001b[34m#011window_size:#011#0115\u001b[0m\n",
      "\u001b[34m#011negative_samples:#0115\u001b[0m\n",
      "\u001b[34m#011alpha:#011#011#0110.025\u001b[0m\n",
      "\u001b[34m#011workers:#011#0114\u001b[0m\n",
      "\u001b[34mStart Training:\u001b[0m\n",
      "\u001b[34m#011Alpha: 0.025000#011Progress: 100.00 %\u001b[0m\n",
      "\u001b[34mSave Model:\u001b[0m\n",
      "\u001b[34m#011Save to </opt/ml/model/rep_dw.txt>\u001b[0m\n",
      "\n",
      "2020-08-20 08:56:20 Uploading - Uploading generated training model\n",
      "2020-08-20 08:56:20 Completed - Training job completed\n",
      "Training seconds: 45\n",
      "Billable seconds: 45\n"
     ]
    }
   ],
   "source": [
    "import boto3 \n",
    "import os \n",
    "import sagemaker \n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "crole = get_execution_role()\n",
    "container = os.environ['docker_name']\n",
    "\n",
    "smore = sagemaker.estimator.Estimator(image_name=container,\n",
    "                                   role=crole, \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=sagemaker.Session())\n",
    "\n",
    "\n",
    "\n",
    "smore.fit({'train': train_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "model_file_name = \"model.tar.gz\"\n",
    "model_full_path = smore.output_path +\"/\"+ smore.latest_training_job.job_name +\"/output/\"+model_file_name\n",
    "print (\"Model Path: \", model_full_path)\n",
    "\n",
    "#Download FM model \n",
    "os.system(\"aws s3 cp \"+model_full_path+ \" .\")\n",
    "\n",
    "#Extract model file for loading to MXNet\n",
    "os.system(\"tar xzvf \"+model_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install annoy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from preprocessing.smore_datareader import SmoreDataReader\n",
    "#to be modified \n",
    "smorereader = SmoreDataReader(transformer.u_idx, transformer.i_idx, \"rep_dw.txt\")\n",
    "user_vectors = smorereader.read_user_data()\n",
    "item_vectors = smorereader.read_item_data()\n",
    "\n",
    "dim = 64\n",
    "t = AnnoyIndex(dim, 'euclidean')  # Length of item vector that will be indexed\n",
    "for k, v in item_vectors.items():\n",
    "    t.add_item(int(k), v)\n",
    "\n",
    "t.build(100)\n",
    "t.save('smore.ann')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "user_item_df = pd.read_pickle(\"data/user_item_df.p\")\n",
    "item_df = pd.read_pickle(\"data/item_df.p\")\n",
    "genres = ['unknown','Action' , 'Adventure', 'Animation', 'Childrens' , 'Comedy' , 'Crime', \\\n",
    "                                        'Documentary', 'Drama' ,'Fantasy' , 'Film-Noir' , 'Horror' , 'Musical', \\\n",
    "                                        'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "\n",
    "user_inv_idx = {} \n",
    "item_inv_idx = {} \n",
    "\n",
    "for k, v in transformer.u_idx.items():\n",
    "    user_inv_idx[v] = k \n",
    "    \n",
    "for k, v in transformer.i_idx.items():\n",
    "    item_inv_idx[v] = k \n",
    "\n",
    "\n",
    "def plot_heat_map(df, figsize=(10,7)): \n",
    "    df = df.div(df.sum(axis=1), axis=0)     \n",
    "    plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(df)\n",
    "\n",
    "\n",
    "# test_user_idx = 89\n",
    "test_user_idx = 200\n",
    "u_id = user_inv_idx[test_user_idx]\n",
    "\n",
    "tester_df = user_item_df[user_item_df['uid']==int(u_id)]\n",
    "tester_df['positive'] = tester_df['rating'] >3 \n",
    "\n",
    "review = tester_df[['positive']+genres].groupby(['positive']).sum()\n",
    "plot_heat_map(review, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "def get_imdb_query(q):\n",
    "    return 'https://www.imdb.com/find?q={}'.format(urllib.parse.quote(q))\n",
    "\n",
    "def print_movie_title(df):\n",
    "    imdb_search_url = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        print (row['title'], get_imdb_query(row['title']))\n",
    "\n",
    "\n",
    "test_positive_df = user_item_df[(user_item_df['uid']==int(u_id)) & (user_item_df['rating']>3)][['title']]\n",
    "print_movie_title(test_positive_df[0:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_negative_df = user_item_df[(user_item_df['uid']==int(u_id)) & (user_item_df['rating']<3)][['title']]\n",
    "print_movie_title(test_negative_df[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns = t.get_nns_by_vector(user_vectors[str(test_user_idx)], 100, search_k=-1, include_distances=False) \n",
    "\n",
    "for movie_id in nns[:10]:\n",
    "    print_movie_title(item_df[item_df['iid']==int(movie_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "item_df = pd.read_pickle(\"data/item_df.p\")\n",
    "genres = ['unknown','Action' , 'Adventure', 'Animation', 'Childrens' , 'Comedy' , 'Crime', \\\n",
    "                                        'Documentary', 'Drama' ,'Fantasy' , 'Film-Noir' , 'Horror' , 'Musical', \\\n",
    "                                        'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "item_genres_df = item_df[['iid']+genres]\n",
    "\n",
    "\n",
    "popularity = user_item_df[user_item_df['rating']>3].groupby('iid').count()['uid'].to_dict() \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def get_answers():\n",
    "    answer = {} \n",
    "    for ui in test_user_item: \n",
    "        uid = ui[0]\n",
    "        iid = ui[1]\n",
    "        rating = ui[2]\n",
    "        if uid not in answer:  \n",
    "            answer[uid] = set() \n",
    "        if rating > 0: \n",
    "            answer[uid].add(iid)\n",
    "    return answer\n",
    "                \n",
    "\n",
    "            \n",
    "def similarity(genre_df1, genre_df2): \n",
    "    sim = 0 \n",
    "    for i, c in enumerate(genre_df1.columns): \n",
    "        if c != 'iid':\n",
    "            if genre_df1[c].iloc[0] == genre_df2[genres][c].iloc[0]: \n",
    "                sim += 1 \n",
    "    return sim / len(genres) \n",
    "           \n",
    "    \n",
    "answer = get_answers() \n",
    "\n",
    "def get_metrics_related_to_recommendation(user_vectors,u_idx,t,answer):\n",
    "    all_pop = 0 \n",
    "    hits = 0 \n",
    "    topk = 10\n",
    "    valid_user_nb = 0 \n",
    "    rcmded = set()\n",
    "    diversity = [] \n",
    "    \n",
    "    for uid in answer.keys():\n",
    "        if uid in u_idx.keys() and uid in user_vectors: \n",
    "            nns = t.get_nns_by_vector(user_vectors[uid], 100, search_k=-1, include_distances=False) \n",
    "            pred = set()\n",
    "            valid_user_nb += 1 \n",
    "            all_sim = []\n",
    "            for i, movieid in enumerate(nns[:topk]): \n",
    "                rcmded.add(movieid)\n",
    "                if movieid in popularity: \n",
    "                    all_pop += popularity[movieid] \n",
    "                pred.add(str(movieid))\n",
    "                for j, movieid_other in enumerate(nns[:topk]):\n",
    "                    if j < i:\n",
    "                        movie_a_df = item_genres_df[item_genres_df['iid'] == int(movieid)]\n",
    "                        movie_b_df = item_genres_df[item_genres_df['iid'] == int(movieid_other)]\n",
    "                        sim = similarity(movie_a_df, movie_b_df)\n",
    "                        all_sim.append(sim) \n",
    "            diversity.append(1/np.mean(np.asarray(all_sim)))             \n",
    "\n",
    "            hits += len(pred.intersection(answer[uid]))\n",
    "    print(\"Novelty:{}\".format(1/(all_pop/topk/valid_user_nb)))\n",
    "    print(\"Hits@10:{}\".format(hits))\n",
    "    print(\"Coverage:{}\".format(len(rcmded)))\n",
    "    print(\"Diversity:{}\".format(np.mean(np.asarray(diversity))))\n",
    "    \n",
    "get_metrics_related_to_recommendation(user_vectors,transformer.u_idx,t,answer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract model data\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from preprocessing.smore_datareader import SmoreDataReader\n",
    "\n",
    "\n",
    "smorereader = SmoreDataReader(transformer.u_idx, transformer.i_idx, \"rep_dw.txt\")\n",
    "user_vectors = smorereader.read_user_data()\n",
    "item_vectors = smorereader.read_item_data()\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "vectors_to_visualize = [] \n",
    "genre_labels = [] \n",
    "for k, v in item_vectors.items(): \n",
    "    iid = k\n",
    "    for index, item_info in item_df[item_df['iid']==int(iid)].iterrows():     \n",
    "        for g in genres: \n",
    "            if item_info[g] > 0:\n",
    "                vectors_to_visualize.append(np.asarray(v)) \n",
    "                genre_labels.append(g)\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "vectors_to_visualize = np.asarray(vectors_to_visualize)                \n",
    "pca = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "pca_result = pca.fit_transform(vectors_to_visualize)\n",
    "d = {} \n",
    "d['pca-1'] = pca_result[:,0]\n",
    "d['pca-2'] = pca_result[:,1]\n",
    "d['genre'] = genre_labels\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-1\", y=\"pca-2\",\n",
    "    hue=\"genre\",\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
