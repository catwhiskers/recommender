{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from preprocessing.imdb_datareader import IMDBDataReader\n",
    "from preprocessing.smore_datareader import SmoreDataReader\n",
    "from preprocessing.xgboost_transformer import  XGBoostTransformer\n",
    "import numpy \n",
    "\n",
    "user_path = './ml-100k/u.user'\n",
    "item_path = './ml-100k/u.item'\n",
    "user_item = './ml-100k/u.data'\n",
    "reader = IMDBDataReader()\n",
    "\n",
    "user_item  = reader.read_user_item_rating(user_item)\n",
    "users = reader.read_user_data(user_path)\n",
    "items = reader.read_item_data(item_path)\n",
    "\n",
    "\n",
    "train_user_item = user_item[:int(len(user_item)*0.8)]\n",
    "test_user_item = user_item[int(len(user_item)*0.8):]\n",
    "\n",
    "\n",
    "smorereader = SmoreDataReader(transformer.u_idx, transformer.i_idx, \"rep_dw.txt\")\n",
    "smore_user_vector = smorereader.read_user_data(\"rep_dw.txt\")\n",
    "\n",
    "total_users = {} \n",
    "for k, v in users.items(): \n",
    "    if k in smore_user_vector: \n",
    "        total_users[k] = v+smore_user_vector[k]\n",
    "\n",
    "transformer = XGBoostTransformer(users, items, train_user_item)\n",
    "X_train, Y_train, _, _, nFeatures = transformer.get_feature_vectors(total_users, items, train_user_item)\n",
    "X_test, Y_test,X_cold_test, Y_cold_test, nFeatures = transformer.get_feature_vectors(total_users, items, test_user_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3api create-bucket --bucket recommendation-demo-yianc-0814 --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: s3://recommendation-demo-yianc/sagemaker/xgboost-movielens/output\n"
     ]
    }
   ],
   "source": [
    "bucket = 'recommendation-demo-yianc'\n",
    "prefix = 'sagemaker/xgboost-movielens'\n",
    "train_key      = 'train'\n",
    "test_key       = 'test'\n",
    "test_cold_key  = 'test_cold'\n",
    "\n",
    "\n",
    "import io,boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeToLocalCSV(X, key, Y): \n",
    "    f_name = key + \".csv\"\n",
    "    output = open(f_name, 'w')\n",
    "\n",
    "\n",
    "    for i in range(0, len(X)): \n",
    "        line = ''\n",
    "        raw = X[i]\n",
    "        line += str(int(Y[i])) \n",
    "        line += ' '\n",
    "        for ele in raw:\n",
    "            line += str(ele[0])+':'+str(float(ele[1]))+' '\n",
    "        line += '\\n'\n",
    "        output.write(line)    \n",
    "    return f_name  \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "train_data = writeToLocalCSV(X_train, train_key, Y_train)    \n",
    "test_data = writeToLocalCSV(X_test, test_key, Y_test)    \n",
    "test_cold_data = writeToLocalCSV(X_cold_test, test_cold_key, Y_cold_test)    \n",
    "  \n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.18.1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:55:07] 80000x109 matrix with 2843231 entries loaded from train.csv\n",
      "[02:55:07] 2863x109 matrix with 102181 entries loaded from test.csv\n",
      "[02:55:07] 89x109 matrix with 3216 entries loaded from test_cold.csv\n",
      "[02:55:07] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix('train.csv')\n",
    "test = xgb.DMatrix('test.csv')\n",
    "test_cold = xgb.DMatrix('test_cold.csv')\n",
    "\n",
    "\n",
    "\n",
    "param = {\n",
    "    'max_depth': 5,  # the maximum depth of each tree\n",
    "    'eta': 0.3,  # the training step for each iteration\n",
    "    'silent': 1,  # logging mode - quiet\n",
    "    'objective': 'multi:softprob',  # error evaluation for multiclass training\n",
    "    'num_class': 2}  # the number of classes that exist in this datset\n",
    "num_round = 3000  # the number of training iterations\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "# dtrain = xgb.DMatrix('train.protobuf.csv')\n",
    "\n",
    "preds = bst.predict(dtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "preds = bst.predict(test)\n",
    "predictions = []\n",
    "for i in range(0, len(preds)):\n",
    "    if preds[i][0] > preds[i][1]:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6685295144952846"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = bst.predict(test_cold)\n",
    "predictions = []\n",
    "for i in range(0, len(preds)):\n",
    "    if preds[i][0] > preds[i][1]:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7078651685393258"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_cold_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
